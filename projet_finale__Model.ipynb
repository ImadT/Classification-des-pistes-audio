{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projet finale_ Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsxCA05CcuzJ8Qq44iMp7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImadT/Classification-des-pistes-audio/blob/master/projet_finale__Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIJHKRvH3EHy",
        "colab_type": "code",
        "outputId": "0c571caa-ce79-4a41-83b7-1135de8e414b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#Drive      \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5wSVCxn3T6H",
        "colab_type": "code",
        "outputId": "059f6ad5-9602-4c95-f104-7e764639345b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "#imports\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "import time \n",
        "import pickle\n",
        "import queue\n",
        "import threading\n",
        "import tensorflow as tf\n",
        "\n",
        "import scipy.io.wavfile as wav\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "from librosa.display import specshow, waveplot\n",
        "import sklearn as skl\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew8VjQD0t62S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('max_info_columns', 999)\n",
        "\n",
        "def swish(x):\n",
        "    return x*K.sigmoid(x)\n",
        "\n",
        "get_custom_objects().update({'custom_activation': swish})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEdK94wg3Z0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Dropout, Input, BatchNormalization, CuDNNLSTM\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping,TensorBoard, ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.metrics import top_k_categorical_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE8CNxlE3exn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data for training\n",
        "features = pd.read_csv(\"/content/drive/My Drive/projet IA/features.csv\", header=[0, 1, 2], index_col=0)\n",
        "classes = pd.read_csv(\"/content/drive/My Drive/projet IA/classes.csv\", index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAAn2e3j3jO3",
        "colab_type": "code",
        "outputId": "178fefa3-6d74-4ca7-f10b-17cbb32ab239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "#afficher matrice de correlation\n",
        "df = pd.DataFrame()\n",
        "\n",
        "result = pd.merge(features, classes['genres'], on='track_id')\n",
        "result.corr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py:617: UserWarning: merging between different levels can give an unintended result (3 levels on the left, 1 on the right)\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(chroma_cens, kurtosis, 01)</th>\n",
              "      <th>(chroma_cens, kurtosis, 02)</th>\n",
              "      <th>(chroma_cens, kurtosis, 03)</th>\n",
              "      <th>(chroma_cens, kurtosis, 04)</th>\n",
              "      <th>(chroma_cens, kurtosis, 05)</th>\n",
              "      <th>(chroma_cens, kurtosis, 06)</th>\n",
              "      <th>(chroma_cens, kurtosis, 07)</th>\n",
              "      <th>(chroma_cens, kurtosis, 08)</th>\n",
              "      <th>(chroma_cens, kurtosis, 09)</th>\n",
              "      <th>(chroma_cens, kurtosis, 10)</th>\n",
              "      <th>(chroma_cens, kurtosis, 11)</th>\n",
              "      <th>(chroma_cens, kurtosis, 12)</th>\n",
              "      <th>(chroma_cens, max, 01)</th>\n",
              "      <th>(chroma_cens, max, 02)</th>\n",
              "      <th>(chroma_cens, max, 03)</th>\n",
              "      <th>(chroma_cens, max, 04)</th>\n",
              "      <th>(chroma_cens, max, 05)</th>\n",
              "      <th>(chroma_cens, max, 06)</th>\n",
              "      <th>(chroma_cens, max, 07)</th>\n",
              "      <th>(chroma_cens, max, 08)</th>\n",
              "      <th>(chroma_cens, max, 09)</th>\n",
              "      <th>(chroma_cens, max, 10)</th>\n",
              "      <th>(chroma_cens, max, 11)</th>\n",
              "      <th>(chroma_cens, max, 12)</th>\n",
              "      <th>(chroma_cens, mean, 01)</th>\n",
              "      <th>(chroma_cens, mean, 02)</th>\n",
              "      <th>(chroma_cens, mean, 03)</th>\n",
              "      <th>(chroma_cens, mean, 04)</th>\n",
              "      <th>(chroma_cens, mean, 05)</th>\n",
              "      <th>(chroma_cens, mean, 06)</th>\n",
              "      <th>(chroma_cens, mean, 07)</th>\n",
              "      <th>(chroma_cens, mean, 08)</th>\n",
              "      <th>(chroma_cens, mean, 09)</th>\n",
              "      <th>(chroma_cens, mean, 10)</th>\n",
              "      <th>(chroma_cens, mean, 11)</th>\n",
              "      <th>(chroma_cens, mean, 12)</th>\n",
              "      <th>(chroma_cens, median, 01)</th>\n",
              "      <th>(chroma_cens, median, 02)</th>\n",
              "      <th>(chroma_cens, median, 03)</th>\n",
              "      <th>(chroma_cens, median, 04)</th>\n",
              "      <th>...</th>\n",
              "      <th>(tonnetz, max, 05)</th>\n",
              "      <th>(tonnetz, max, 06)</th>\n",
              "      <th>(tonnetz, mean, 01)</th>\n",
              "      <th>(tonnetz, mean, 02)</th>\n",
              "      <th>(tonnetz, mean, 03)</th>\n",
              "      <th>(tonnetz, mean, 04)</th>\n",
              "      <th>(tonnetz, mean, 05)</th>\n",
              "      <th>(tonnetz, mean, 06)</th>\n",
              "      <th>(tonnetz, median, 01)</th>\n",
              "      <th>(tonnetz, median, 02)</th>\n",
              "      <th>(tonnetz, median, 03)</th>\n",
              "      <th>(tonnetz, median, 04)</th>\n",
              "      <th>(tonnetz, median, 05)</th>\n",
              "      <th>(tonnetz, median, 06)</th>\n",
              "      <th>(tonnetz, min, 01)</th>\n",
              "      <th>(tonnetz, min, 02)</th>\n",
              "      <th>(tonnetz, min, 03)</th>\n",
              "      <th>(tonnetz, min, 04)</th>\n",
              "      <th>(tonnetz, min, 05)</th>\n",
              "      <th>(tonnetz, min, 06)</th>\n",
              "      <th>(tonnetz, skew, 01)</th>\n",
              "      <th>(tonnetz, skew, 02)</th>\n",
              "      <th>(tonnetz, skew, 03)</th>\n",
              "      <th>(tonnetz, skew, 04)</th>\n",
              "      <th>(tonnetz, skew, 05)</th>\n",
              "      <th>(tonnetz, skew, 06)</th>\n",
              "      <th>(tonnetz, std, 01)</th>\n",
              "      <th>(tonnetz, std, 02)</th>\n",
              "      <th>(tonnetz, std, 03)</th>\n",
              "      <th>(tonnetz, std, 04)</th>\n",
              "      <th>(tonnetz, std, 05)</th>\n",
              "      <th>(tonnetz, std, 06)</th>\n",
              "      <th>(zcr, kurtosis, 01)</th>\n",
              "      <th>(zcr, max, 01)</th>\n",
              "      <th>(zcr, mean, 01)</th>\n",
              "      <th>(zcr, median, 01)</th>\n",
              "      <th>(zcr, min, 01)</th>\n",
              "      <th>(zcr, skew, 01)</th>\n",
              "      <th>(zcr, std, 01)</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(chroma_cens, kurtosis, 01)</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333799</td>\n",
              "      <td>0.047314</td>\n",
              "      <td>0.183893</td>\n",
              "      <td>0.216923</td>\n",
              "      <td>0.209492</td>\n",
              "      <td>0.145475</td>\n",
              "      <td>0.223332</td>\n",
              "      <td>0.221560</td>\n",
              "      <td>0.172495</td>\n",
              "      <td>0.095915</td>\n",
              "      <td>0.377190</td>\n",
              "      <td>-0.127303</td>\n",
              "      <td>-0.130356</td>\n",
              "      <td>-0.042837</td>\n",
              "      <td>-0.061894</td>\n",
              "      <td>-0.119776</td>\n",
              "      <td>-0.095361</td>\n",
              "      <td>-0.087380</td>\n",
              "      <td>-0.096394</td>\n",
              "      <td>-0.111574</td>\n",
              "      <td>-0.078037</td>\n",
              "      <td>-0.059751</td>\n",
              "      <td>-0.161800</td>\n",
              "      <td>-0.245684</td>\n",
              "      <td>-0.043549</td>\n",
              "      <td>0.138818</td>\n",
              "      <td>0.130309</td>\n",
              "      <td>0.002077</td>\n",
              "      <td>0.012888</td>\n",
              "      <td>0.044245</td>\n",
              "      <td>0.019963</td>\n",
              "      <td>0.043331</td>\n",
              "      <td>0.086817</td>\n",
              "      <td>0.038617</td>\n",
              "      <td>-0.168105</td>\n",
              "      <td>-0.224816</td>\n",
              "      <td>-0.033636</td>\n",
              "      <td>0.147037</td>\n",
              "      <td>0.141442</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063563</td>\n",
              "      <td>-0.065267</td>\n",
              "      <td>-0.061504</td>\n",
              "      <td>-0.046875</td>\n",
              "      <td>-0.032625</td>\n",
              "      <td>-0.185505</td>\n",
              "      <td>-0.026727</td>\n",
              "      <td>-0.049212</td>\n",
              "      <td>-0.059399</td>\n",
              "      <td>-0.047406</td>\n",
              "      <td>-0.041224</td>\n",
              "      <td>-0.189094</td>\n",
              "      <td>-0.027284</td>\n",
              "      <td>-0.059348</td>\n",
              "      <td>0.020956</td>\n",
              "      <td>0.043772</td>\n",
              "      <td>0.114543</td>\n",
              "      <td>0.035711</td>\n",
              "      <td>0.076226</td>\n",
              "      <td>0.056630</td>\n",
              "      <td>0.005353</td>\n",
              "      <td>0.028870</td>\n",
              "      <td>0.035398</td>\n",
              "      <td>0.070806</td>\n",
              "      <td>0.030786</td>\n",
              "      <td>0.039132</td>\n",
              "      <td>-0.049924</td>\n",
              "      <td>-0.105854</td>\n",
              "      <td>-0.186637</td>\n",
              "      <td>-0.182373</td>\n",
              "      <td>-0.151990</td>\n",
              "      <td>-0.136600</td>\n",
              "      <td>-0.008573</td>\n",
              "      <td>-0.028872</td>\n",
              "      <td>0.033587</td>\n",
              "      <td>0.045052</td>\n",
              "      <td>-0.002953</td>\n",
              "      <td>-0.026875</td>\n",
              "      <td>-0.009319</td>\n",
              "      <td>-0.016925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(chroma_cens, kurtosis, 02)</th>\n",
              "      <td>0.333799</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>0.294354</td>\n",
              "      <td>0.284165</td>\n",
              "      <td>0.172308</td>\n",
              "      <td>0.139419</td>\n",
              "      <td>0.178085</td>\n",
              "      <td>0.320730</td>\n",
              "      <td>0.167317</td>\n",
              "      <td>0.165494</td>\n",
              "      <td>0.312164</td>\n",
              "      <td>-0.162722</td>\n",
              "      <td>-0.094354</td>\n",
              "      <td>-0.163989</td>\n",
              "      <td>-0.064117</td>\n",
              "      <td>-0.089758</td>\n",
              "      <td>-0.117102</td>\n",
              "      <td>-0.110870</td>\n",
              "      <td>-0.166905</td>\n",
              "      <td>-0.092834</td>\n",
              "      <td>-0.118315</td>\n",
              "      <td>-0.077581</td>\n",
              "      <td>-0.081004</td>\n",
              "      <td>-0.033409</td>\n",
              "      <td>-0.083096</td>\n",
              "      <td>-0.024461</td>\n",
              "      <td>0.086235</td>\n",
              "      <td>0.093267</td>\n",
              "      <td>0.053371</td>\n",
              "      <td>0.004635</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>0.009321</td>\n",
              "      <td>0.035292</td>\n",
              "      <td>0.029344</td>\n",
              "      <td>-0.016502</td>\n",
              "      <td>-0.077879</td>\n",
              "      <td>-0.009411</td>\n",
              "      <td>0.083751</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070647</td>\n",
              "      <td>-0.051233</td>\n",
              "      <td>-0.022421</td>\n",
              "      <td>0.020044</td>\n",
              "      <td>0.060252</td>\n",
              "      <td>0.025944</td>\n",
              "      <td>0.003960</td>\n",
              "      <td>-0.006218</td>\n",
              "      <td>-0.019680</td>\n",
              "      <td>0.022852</td>\n",
              "      <td>0.062171</td>\n",
              "      <td>0.020743</td>\n",
              "      <td>0.005596</td>\n",
              "      <td>-0.012503</td>\n",
              "      <td>0.060582</td>\n",
              "      <td>0.079657</td>\n",
              "      <td>0.137756</td>\n",
              "      <td>0.139026</td>\n",
              "      <td>0.094728</td>\n",
              "      <td>0.070181</td>\n",
              "      <td>0.004967</td>\n",
              "      <td>0.027977</td>\n",
              "      <td>-0.049611</td>\n",
              "      <td>-0.038214</td>\n",
              "      <td>0.012357</td>\n",
              "      <td>0.033389</td>\n",
              "      <td>-0.098098</td>\n",
              "      <td>-0.099957</td>\n",
              "      <td>-0.187051</td>\n",
              "      <td>-0.196685</td>\n",
              "      <td>-0.160046</td>\n",
              "      <td>-0.144811</td>\n",
              "      <td>-0.009148</td>\n",
              "      <td>-0.030996</td>\n",
              "      <td>0.058468</td>\n",
              "      <td>0.068913</td>\n",
              "      <td>0.014279</td>\n",
              "      <td>-0.042154</td>\n",
              "      <td>-0.003265</td>\n",
              "      <td>-0.041775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(chroma_cens, kurtosis, 03)</th>\n",
              "      <td>0.047314</td>\n",
              "      <td>0.090431</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.136519</td>\n",
              "      <td>0.061075</td>\n",
              "      <td>0.036194</td>\n",
              "      <td>0.066842</td>\n",
              "      <td>0.076790</td>\n",
              "      <td>0.038855</td>\n",
              "      <td>0.053856</td>\n",
              "      <td>0.030409</td>\n",
              "      <td>0.040836</td>\n",
              "      <td>-0.004493</td>\n",
              "      <td>-0.056530</td>\n",
              "      <td>-0.080601</td>\n",
              "      <td>-0.052420</td>\n",
              "      <td>-0.008835</td>\n",
              "      <td>-0.013407</td>\n",
              "      <td>-0.041113</td>\n",
              "      <td>-0.076965</td>\n",
              "      <td>-0.020840</td>\n",
              "      <td>-0.008470</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>-0.004471</td>\n",
              "      <td>0.023740</td>\n",
              "      <td>-0.041048</td>\n",
              "      <td>-0.097257</td>\n",
              "      <td>-0.062046</td>\n",
              "      <td>0.010339</td>\n",
              "      <td>0.019579</td>\n",
              "      <td>-0.023322</td>\n",
              "      <td>-0.037417</td>\n",
              "      <td>-0.003811</td>\n",
              "      <td>0.031510</td>\n",
              "      <td>0.036141</td>\n",
              "      <td>0.036039</td>\n",
              "      <td>0.018671</td>\n",
              "      <td>-0.049200</td>\n",
              "      <td>-0.085218</td>\n",
              "      <td>-0.061595</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000173</td>\n",
              "      <td>0.007312</td>\n",
              "      <td>-0.015577</td>\n",
              "      <td>-0.011694</td>\n",
              "      <td>-0.038698</td>\n",
              "      <td>0.053015</td>\n",
              "      <td>0.028745</td>\n",
              "      <td>0.030394</td>\n",
              "      <td>-0.016757</td>\n",
              "      <td>-0.012834</td>\n",
              "      <td>-0.033921</td>\n",
              "      <td>0.060062</td>\n",
              "      <td>0.032565</td>\n",
              "      <td>0.033477</td>\n",
              "      <td>-0.004836</td>\n",
              "      <td>-0.004855</td>\n",
              "      <td>0.010114</td>\n",
              "      <td>0.033383</td>\n",
              "      <td>0.004478</td>\n",
              "      <td>0.008677</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.007093</td>\n",
              "      <td>0.005952</td>\n",
              "      <td>-0.033955</td>\n",
              "      <td>-0.028242</td>\n",
              "      <td>-0.009514</td>\n",
              "      <td>0.019665</td>\n",
              "      <td>0.014425</td>\n",
              "      <td>-0.039860</td>\n",
              "      <td>-0.025665</td>\n",
              "      <td>-0.013930</td>\n",
              "      <td>-0.002361</td>\n",
              "      <td>-0.008530</td>\n",
              "      <td>-0.020845</td>\n",
              "      <td>-0.005047</td>\n",
              "      <td>-0.002326</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>-0.014647</td>\n",
              "      <td>-0.008898</td>\n",
              "      <td>0.000273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(chroma_cens, kurtosis, 04)</th>\n",
              "      <td>0.183893</td>\n",
              "      <td>0.294354</td>\n",
              "      <td>0.136519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.368300</td>\n",
              "      <td>0.221327</td>\n",
              "      <td>0.159154</td>\n",
              "      <td>0.143106</td>\n",
              "      <td>0.255472</td>\n",
              "      <td>0.125954</td>\n",
              "      <td>0.107491</td>\n",
              "      <td>0.148100</td>\n",
              "      <td>-0.071227</td>\n",
              "      <td>-0.066088</td>\n",
              "      <td>-0.183150</td>\n",
              "      <td>-0.049681</td>\n",
              "      <td>-0.191094</td>\n",
              "      <td>-0.105923</td>\n",
              "      <td>-0.121334</td>\n",
              "      <td>-0.142997</td>\n",
              "      <td>-0.088380</td>\n",
              "      <td>-0.153382</td>\n",
              "      <td>-0.047036</td>\n",
              "      <td>-0.114312</td>\n",
              "      <td>0.122563</td>\n",
              "      <td>0.086681</td>\n",
              "      <td>-0.034123</td>\n",
              "      <td>-0.108663</td>\n",
              "      <td>-0.042539</td>\n",
              "      <td>0.047429</td>\n",
              "      <td>0.069354</td>\n",
              "      <td>0.052328</td>\n",
              "      <td>0.034952</td>\n",
              "      <td>0.019249</td>\n",
              "      <td>0.026143</td>\n",
              "      <td>0.064985</td>\n",
              "      <td>0.140332</td>\n",
              "      <td>0.086599</td>\n",
              "      <td>-0.012979</td>\n",
              "      <td>-0.108602</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065877</td>\n",
              "      <td>-0.055075</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>0.051992</td>\n",
              "      <td>-0.055165</td>\n",
              "      <td>0.027244</td>\n",
              "      <td>0.013990</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>-0.003013</td>\n",
              "      <td>0.050461</td>\n",
              "      <td>-0.059672</td>\n",
              "      <td>0.032477</td>\n",
              "      <td>0.016476</td>\n",
              "      <td>-0.003229</td>\n",
              "      <td>0.085908</td>\n",
              "      <td>0.080079</td>\n",
              "      <td>0.134604</td>\n",
              "      <td>0.145630</td>\n",
              "      <td>0.109462</td>\n",
              "      <td>0.063038</td>\n",
              "      <td>0.009604</td>\n",
              "      <td>0.040262</td>\n",
              "      <td>0.063584</td>\n",
              "      <td>-0.048124</td>\n",
              "      <td>0.019736</td>\n",
              "      <td>0.007023</td>\n",
              "      <td>-0.153152</td>\n",
              "      <td>-0.098952</td>\n",
              "      <td>-0.225804</td>\n",
              "      <td>-0.241133</td>\n",
              "      <td>-0.190101</td>\n",
              "      <td>-0.178557</td>\n",
              "      <td>-0.022580</td>\n",
              "      <td>-0.017248</td>\n",
              "      <td>0.091038</td>\n",
              "      <td>0.100325</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>-0.057835</td>\n",
              "      <td>0.012334</td>\n",
              "      <td>-0.037266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(chroma_cens, kurtosis, 05)</th>\n",
              "      <td>0.216923</td>\n",
              "      <td>0.284165</td>\n",
              "      <td>0.061075</td>\n",
              "      <td>0.368300</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.455206</td>\n",
              "      <td>0.199415</td>\n",
              "      <td>0.119762</td>\n",
              "      <td>0.196315</td>\n",
              "      <td>0.150748</td>\n",
              "      <td>0.114309</td>\n",
              "      <td>0.234146</td>\n",
              "      <td>-0.105949</td>\n",
              "      <td>-0.059116</td>\n",
              "      <td>-0.072912</td>\n",
              "      <td>-0.144646</td>\n",
              "      <td>-0.122957</td>\n",
              "      <td>-0.174708</td>\n",
              "      <td>-0.070363</td>\n",
              "      <td>-0.115281</td>\n",
              "      <td>-0.099717</td>\n",
              "      <td>-0.092020</td>\n",
              "      <td>-0.083337</td>\n",
              "      <td>-0.056258</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>0.100921</td>\n",
              "      <td>0.109409</td>\n",
              "      <td>-0.046618</td>\n",
              "      <td>-0.197695</td>\n",
              "      <td>-0.117357</td>\n",
              "      <td>0.046475</td>\n",
              "      <td>0.087277</td>\n",
              "      <td>0.073247</td>\n",
              "      <td>0.039556</td>\n",
              "      <td>0.030389</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>-0.000070</td>\n",
              "      <td>0.103360</td>\n",
              "      <td>0.122359</td>\n",
              "      <td>-0.032643</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079754</td>\n",
              "      <td>-0.067977</td>\n",
              "      <td>-0.044635</td>\n",
              "      <td>-0.005433</td>\n",
              "      <td>0.010936</td>\n",
              "      <td>-0.134792</td>\n",
              "      <td>-0.081045</td>\n",
              "      <td>-0.002332</td>\n",
              "      <td>-0.050069</td>\n",
              "      <td>-0.007198</td>\n",
              "      <td>0.003787</td>\n",
              "      <td>-0.144927</td>\n",
              "      <td>-0.085513</td>\n",
              "      <td>-0.008767</td>\n",
              "      <td>0.045389</td>\n",
              "      <td>0.033586</td>\n",
              "      <td>0.141427</td>\n",
              "      <td>0.069878</td>\n",
              "      <td>0.066608</td>\n",
              "      <td>0.061093</td>\n",
              "      <td>0.006123</td>\n",
              "      <td>0.004426</td>\n",
              "      <td>0.026930</td>\n",
              "      <td>0.041247</td>\n",
              "      <td>0.065725</td>\n",
              "      <td>0.005391</td>\n",
              "      <td>-0.097479</td>\n",
              "      <td>-0.069066</td>\n",
              "      <td>-0.193617</td>\n",
              "      <td>-0.202567</td>\n",
              "      <td>-0.153066</td>\n",
              "      <td>-0.155795</td>\n",
              "      <td>-0.001363</td>\n",
              "      <td>-0.028221</td>\n",
              "      <td>0.035483</td>\n",
              "      <td>0.047916</td>\n",
              "      <td>-0.000837</td>\n",
              "      <td>-0.023603</td>\n",
              "      <td>-0.015832</td>\n",
              "      <td>-0.028789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(zcr, median, 01)</th>\n",
              "      <td>0.045052</td>\n",
              "      <td>0.068913</td>\n",
              "      <td>-0.002326</td>\n",
              "      <td>0.100325</td>\n",
              "      <td>0.047916</td>\n",
              "      <td>0.053088</td>\n",
              "      <td>0.053866</td>\n",
              "      <td>0.008212</td>\n",
              "      <td>0.060986</td>\n",
              "      <td>0.029698</td>\n",
              "      <td>0.009963</td>\n",
              "      <td>0.025614</td>\n",
              "      <td>-0.148750</td>\n",
              "      <td>-0.132604</td>\n",
              "      <td>-0.146351</td>\n",
              "      <td>-0.142111</td>\n",
              "      <td>-0.118145</td>\n",
              "      <td>-0.132769</td>\n",
              "      <td>-0.118441</td>\n",
              "      <td>-0.141877</td>\n",
              "      <td>-0.135728</td>\n",
              "      <td>-0.117853</td>\n",
              "      <td>-0.133561</td>\n",
              "      <td>-0.100273</td>\n",
              "      <td>0.070911</td>\n",
              "      <td>0.076329</td>\n",
              "      <td>0.077741</td>\n",
              "      <td>0.095373</td>\n",
              "      <td>0.108424</td>\n",
              "      <td>0.121463</td>\n",
              "      <td>0.109890</td>\n",
              "      <td>0.089459</td>\n",
              "      <td>0.108401</td>\n",
              "      <td>0.122611</td>\n",
              "      <td>0.149596</td>\n",
              "      <td>0.146902</td>\n",
              "      <td>0.086751</td>\n",
              "      <td>0.081952</td>\n",
              "      <td>0.092892</td>\n",
              "      <td>0.102077</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.205228</td>\n",
              "      <td>-0.202583</td>\n",
              "      <td>0.045148</td>\n",
              "      <td>-0.041406</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>-0.016016</td>\n",
              "      <td>-0.031776</td>\n",
              "      <td>-0.050371</td>\n",
              "      <td>0.048475</td>\n",
              "      <td>-0.047860</td>\n",
              "      <td>0.002468</td>\n",
              "      <td>-0.016939</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>-0.059721</td>\n",
              "      <td>0.210989</td>\n",
              "      <td>0.216776</td>\n",
              "      <td>0.225004</td>\n",
              "      <td>0.201802</td>\n",
              "      <td>0.208275</td>\n",
              "      <td>0.177615</td>\n",
              "      <td>-0.022975</td>\n",
              "      <td>0.077397</td>\n",
              "      <td>0.008144</td>\n",
              "      <td>0.007883</td>\n",
              "      <td>0.018064</td>\n",
              "      <td>0.028770</td>\n",
              "      <td>-0.369637</td>\n",
              "      <td>-0.361705</td>\n",
              "      <td>-0.330422</td>\n",
              "      <td>-0.324748</td>\n",
              "      <td>-0.349183</td>\n",
              "      <td>-0.353846</td>\n",
              "      <td>-0.196111</td>\n",
              "      <td>0.132330</td>\n",
              "      <td>0.936524</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.314266</td>\n",
              "      <td>-0.362515</td>\n",
              "      <td>0.331326</td>\n",
              "      <td>-0.191111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(zcr, min, 01)</th>\n",
              "      <td>-0.002953</td>\n",
              "      <td>0.014279</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>-0.000837</td>\n",
              "      <td>0.008904</td>\n",
              "      <td>0.037953</td>\n",
              "      <td>-0.001703</td>\n",
              "      <td>-0.010689</td>\n",
              "      <td>0.011585</td>\n",
              "      <td>0.004642</td>\n",
              "      <td>-0.017054</td>\n",
              "      <td>-0.186706</td>\n",
              "      <td>-0.121832</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>-0.087584</td>\n",
              "      <td>-0.060677</td>\n",
              "      <td>-0.090337</td>\n",
              "      <td>-0.071774</td>\n",
              "      <td>-0.107585</td>\n",
              "      <td>-0.096560</td>\n",
              "      <td>-0.087471</td>\n",
              "      <td>-0.099706</td>\n",
              "      <td>-0.072318</td>\n",
              "      <td>-0.008262</td>\n",
              "      <td>-0.010147</td>\n",
              "      <td>0.004157</td>\n",
              "      <td>0.025203</td>\n",
              "      <td>0.048774</td>\n",
              "      <td>0.044556</td>\n",
              "      <td>0.033063</td>\n",
              "      <td>0.024931</td>\n",
              "      <td>0.031852</td>\n",
              "      <td>0.040702</td>\n",
              "      <td>0.069753</td>\n",
              "      <td>0.097089</td>\n",
              "      <td>-0.003306</td>\n",
              "      <td>-0.010727</td>\n",
              "      <td>0.003853</td>\n",
              "      <td>0.019311</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.151432</td>\n",
              "      <td>-0.169054</td>\n",
              "      <td>0.050187</td>\n",
              "      <td>-0.100452</td>\n",
              "      <td>0.026262</td>\n",
              "      <td>-0.011201</td>\n",
              "      <td>-0.044118</td>\n",
              "      <td>-0.079505</td>\n",
              "      <td>0.049571</td>\n",
              "      <td>-0.098452</td>\n",
              "      <td>0.027688</td>\n",
              "      <td>-0.009948</td>\n",
              "      <td>-0.044677</td>\n",
              "      <td>-0.077396</td>\n",
              "      <td>0.159708</td>\n",
              "      <td>0.123985</td>\n",
              "      <td>0.105229</td>\n",
              "      <td>0.086104</td>\n",
              "      <td>0.101538</td>\n",
              "      <td>0.105908</td>\n",
              "      <td>-0.000737</td>\n",
              "      <td>-0.065090</td>\n",
              "      <td>-0.011105</td>\n",
              "      <td>-0.012381</td>\n",
              "      <td>0.003607</td>\n",
              "      <td>0.005530</td>\n",
              "      <td>-0.115974</td>\n",
              "      <td>-0.157609</td>\n",
              "      <td>-0.060123</td>\n",
              "      <td>-0.056512</td>\n",
              "      <td>-0.076998</td>\n",
              "      <td>-0.091982</td>\n",
              "      <td>0.042920</td>\n",
              "      <td>-0.014343</td>\n",
              "      <td>0.260580</td>\n",
              "      <td>0.314266</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007259</td>\n",
              "      <td>-0.047815</td>\n",
              "      <td>0.012501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(zcr, skew, 01)</th>\n",
              "      <td>-0.026875</td>\n",
              "      <td>-0.042154</td>\n",
              "      <td>-0.014647</td>\n",
              "      <td>-0.057835</td>\n",
              "      <td>-0.023603</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.034288</td>\n",
              "      <td>-0.030090</td>\n",
              "      <td>-0.046899</td>\n",
              "      <td>-0.022305</td>\n",
              "      <td>0.017876</td>\n",
              "      <td>-0.023813</td>\n",
              "      <td>0.042946</td>\n",
              "      <td>0.046829</td>\n",
              "      <td>0.077565</td>\n",
              "      <td>0.079539</td>\n",
              "      <td>0.067100</td>\n",
              "      <td>0.053203</td>\n",
              "      <td>0.062048</td>\n",
              "      <td>0.086105</td>\n",
              "      <td>0.065425</td>\n",
              "      <td>0.057553</td>\n",
              "      <td>0.056904</td>\n",
              "      <td>0.053613</td>\n",
              "      <td>-0.075105</td>\n",
              "      <td>-0.080550</td>\n",
              "      <td>-0.047995</td>\n",
              "      <td>-0.042061</td>\n",
              "      <td>-0.055640</td>\n",
              "      <td>-0.078279</td>\n",
              "      <td>-0.067240</td>\n",
              "      <td>-0.040327</td>\n",
              "      <td>-0.058243</td>\n",
              "      <td>-0.077660</td>\n",
              "      <td>-0.083008</td>\n",
              "      <td>-0.061257</td>\n",
              "      <td>-0.086648</td>\n",
              "      <td>-0.085730</td>\n",
              "      <td>-0.060321</td>\n",
              "      <td>-0.051290</td>\n",
              "      <td>...</td>\n",
              "      <td>0.119044</td>\n",
              "      <td>0.115817</td>\n",
              "      <td>0.003642</td>\n",
              "      <td>-0.018344</td>\n",
              "      <td>0.038799</td>\n",
              "      <td>-0.000074</td>\n",
              "      <td>-0.012589</td>\n",
              "      <td>-0.015452</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>-0.017276</td>\n",
              "      <td>0.038971</td>\n",
              "      <td>0.003026</td>\n",
              "      <td>-0.010958</td>\n",
              "      <td>-0.007582</td>\n",
              "      <td>-0.119824</td>\n",
              "      <td>-0.121435</td>\n",
              "      <td>-0.127663</td>\n",
              "      <td>-0.138896</td>\n",
              "      <td>-0.141401</td>\n",
              "      <td>-0.124096</td>\n",
              "      <td>-0.000614</td>\n",
              "      <td>-0.036780</td>\n",
              "      <td>-0.005460</td>\n",
              "      <td>-0.010463</td>\n",
              "      <td>-0.012129</td>\n",
              "      <td>-0.006403</td>\n",
              "      <td>0.241759</td>\n",
              "      <td>0.223524</td>\n",
              "      <td>0.235752</td>\n",
              "      <td>0.243768</td>\n",
              "      <td>0.251667</td>\n",
              "      <td>0.258965</td>\n",
              "      <td>0.849865</td>\n",
              "      <td>0.478248</td>\n",
              "      <td>-0.366380</td>\n",
              "      <td>-0.362515</td>\n",
              "      <td>0.007259</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.012133</td>\n",
              "      <td>0.155226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(zcr, std, 01)</th>\n",
              "      <td>-0.009319</td>\n",
              "      <td>-0.003265</td>\n",
              "      <td>-0.008898</td>\n",
              "      <td>0.012334</td>\n",
              "      <td>-0.015832</td>\n",
              "      <td>0.007804</td>\n",
              "      <td>-0.009769</td>\n",
              "      <td>-0.022992</td>\n",
              "      <td>-0.009648</td>\n",
              "      <td>-0.000636</td>\n",
              "      <td>-0.013307</td>\n",
              "      <td>0.016690</td>\n",
              "      <td>0.029523</td>\n",
              "      <td>0.037031</td>\n",
              "      <td>-0.039643</td>\n",
              "      <td>0.009404</td>\n",
              "      <td>-0.023584</td>\n",
              "      <td>0.029477</td>\n",
              "      <td>0.022458</td>\n",
              "      <td>0.004774</td>\n",
              "      <td>0.044437</td>\n",
              "      <td>-0.006583</td>\n",
              "      <td>0.013200</td>\n",
              "      <td>-0.016634</td>\n",
              "      <td>0.031174</td>\n",
              "      <td>0.011961</td>\n",
              "      <td>-0.009644</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>0.016042</td>\n",
              "      <td>0.062050</td>\n",
              "      <td>0.066301</td>\n",
              "      <td>0.037964</td>\n",
              "      <td>0.046460</td>\n",
              "      <td>0.028524</td>\n",
              "      <td>0.025807</td>\n",
              "      <td>-0.018068</td>\n",
              "      <td>0.037529</td>\n",
              "      <td>0.014338</td>\n",
              "      <td>0.003432</td>\n",
              "      <td>0.009836</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066614</td>\n",
              "      <td>-0.017931</td>\n",
              "      <td>-0.096817</td>\n",
              "      <td>0.072589</td>\n",
              "      <td>-0.030369</td>\n",
              "      <td>0.006629</td>\n",
              "      <td>0.017397</td>\n",
              "      <td>0.051612</td>\n",
              "      <td>-0.093274</td>\n",
              "      <td>0.063983</td>\n",
              "      <td>-0.036201</td>\n",
              "      <td>0.006729</td>\n",
              "      <td>0.020738</td>\n",
              "      <td>0.039709</td>\n",
              "      <td>0.026845</td>\n",
              "      <td>0.078156</td>\n",
              "      <td>0.063695</td>\n",
              "      <td>0.077887</td>\n",
              "      <td>0.089901</td>\n",
              "      <td>0.089315</td>\n",
              "      <td>-0.008119</td>\n",
              "      <td>0.068111</td>\n",
              "      <td>0.030639</td>\n",
              "      <td>0.007512</td>\n",
              "      <td>-0.005631</td>\n",
              "      <td>0.041574</td>\n",
              "      <td>-0.095852</td>\n",
              "      <td>-0.053508</td>\n",
              "      <td>-0.148021</td>\n",
              "      <td>-0.145252</td>\n",
              "      <td>-0.161750</td>\n",
              "      <td>-0.139124</td>\n",
              "      <td>-0.120893</td>\n",
              "      <td>0.689577</td>\n",
              "      <td>0.599569</td>\n",
              "      <td>0.331326</td>\n",
              "      <td>-0.047815</td>\n",
              "      <td>-0.012133</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.139438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genres</th>\n",
              "      <td>-0.016925</td>\n",
              "      <td>-0.041775</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>-0.037266</td>\n",
              "      <td>-0.028789</td>\n",
              "      <td>0.003298</td>\n",
              "      <td>-0.041154</td>\n",
              "      <td>-0.008650</td>\n",
              "      <td>-0.041249</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>-0.005304</td>\n",
              "      <td>-0.022943</td>\n",
              "      <td>0.067112</td>\n",
              "      <td>-0.009229</td>\n",
              "      <td>0.050647</td>\n",
              "      <td>0.002994</td>\n",
              "      <td>-0.001416</td>\n",
              "      <td>0.037570</td>\n",
              "      <td>-0.035354</td>\n",
              "      <td>0.053714</td>\n",
              "      <td>-0.010314</td>\n",
              "      <td>0.027090</td>\n",
              "      <td>0.011073</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>-0.003154</td>\n",
              "      <td>-0.021013</td>\n",
              "      <td>-0.049008</td>\n",
              "      <td>-0.085058</td>\n",
              "      <td>-0.077678</td>\n",
              "      <td>-0.088024</td>\n",
              "      <td>-0.074777</td>\n",
              "      <td>-0.042558</td>\n",
              "      <td>-0.061077</td>\n",
              "      <td>-0.075564</td>\n",
              "      <td>-0.102894</td>\n",
              "      <td>-0.034946</td>\n",
              "      <td>-0.013652</td>\n",
              "      <td>-0.017950</td>\n",
              "      <td>-0.059644</td>\n",
              "      <td>-0.084192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.144498</td>\n",
              "      <td>0.061527</td>\n",
              "      <td>-0.016677</td>\n",
              "      <td>0.008959</td>\n",
              "      <td>0.033577</td>\n",
              "      <td>-0.013121</td>\n",
              "      <td>0.007020</td>\n",
              "      <td>0.056695</td>\n",
              "      <td>-0.014419</td>\n",
              "      <td>0.012166</td>\n",
              "      <td>0.032681</td>\n",
              "      <td>-0.015981</td>\n",
              "      <td>0.015685</td>\n",
              "      <td>-0.105005</td>\n",
              "      <td>-0.153286</td>\n",
              "      <td>-0.137102</td>\n",
              "      <td>-0.099891</td>\n",
              "      <td>-0.155368</td>\n",
              "      <td>-0.115684</td>\n",
              "      <td>0.023972</td>\n",
              "      <td>-0.036053</td>\n",
              "      <td>-0.028724</td>\n",
              "      <td>-0.002313</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>0.005709</td>\n",
              "      <td>0.273037</td>\n",
              "      <td>0.261812</td>\n",
              "      <td>0.235162</td>\n",
              "      <td>0.230967</td>\n",
              "      <td>0.282502</td>\n",
              "      <td>0.275760</td>\n",
              "      <td>0.132882</td>\n",
              "      <td>-0.121027</td>\n",
              "      <td>-0.215573</td>\n",
              "      <td>-0.191111</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>0.155226</td>\n",
              "      <td>-0.139438</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>519 rows × 519 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             (chroma_cens, kurtosis, 01)  ...    genres\n",
              "(chroma_cens, kurtosis, 01)                     1.000000  ... -0.016925\n",
              "(chroma_cens, kurtosis, 02)                     0.333799  ... -0.041775\n",
              "(chroma_cens, kurtosis, 03)                     0.047314  ...  0.000273\n",
              "(chroma_cens, kurtosis, 04)                     0.183893  ... -0.037266\n",
              "(chroma_cens, kurtosis, 05)                     0.216923  ... -0.028789\n",
              "...                                                  ...  ...       ...\n",
              "(zcr, median, 01)                               0.045052  ... -0.191111\n",
              "(zcr, min, 01)                                 -0.002953  ...  0.012501\n",
              "(zcr, skew, 01)                                -0.026875  ...  0.155226\n",
              "(zcr, std, 01)                                 -0.009319  ... -0.139438\n",
              "genres                                         -0.016925  ...  1.000000\n",
              "\n",
              "[519 rows x 519 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDc5GEZS3zw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.to_csv(\"/content/drive/My Drive/projet IA/result.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0e9DXoK34uR",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFWeqD8CfZH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to fit the output\n",
        "label = pd.read_csv(\"/content/drive/My Drive/projet IA/classes.csv\", index_col = 0)\n",
        "\n",
        "enc = LabelBinarizer()\n",
        "y_enc = enc.fit_transform(label.genres.values.reshape(-1, 1)) # .title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLhDV_1EfywJ",
        "colab_type": "code",
        "outputId": "d389f9d6-1564-4a56-d252-9377b7de3634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "y_enc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOqSz1s39L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "#This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.\n",
        "X = MinMaxScaler().fit_transform(features)\n",
        "# phase ACP\n",
        "pca = PCA(n_components=0.95)\n",
        "new_X = pca.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "102v-bkS9KGV",
        "colab_type": "code",
        "outputId": "60afa0c9-6399-4815-b3b5-299a56a5a881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgPI3m74sPpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split of data\n",
        "X_train, X_test, y_train, y_test, = train_test_split(new_X, y_enc, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhhh_r9PtQH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Dropout, Input, BatchNormalization, CuDNNLSTM\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.metrics import top_k_categorical_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLaZ_sqTtUHY",
        "colab_type": "code",
        "outputId": "32e6b24d-6d16-478d-da5b-2aacb1b23cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "#relu\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               19072     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 8)                 136       \n",
            "=================================================================\n",
            "Total params: 30,072\n",
            "Trainable params: 30,072\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahgd6PfbuCHc",
        "colab_type": "code",
        "outputId": "045f4755-290d-4ead-db88-7d9d7ec2d708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='Nadam', \n",
        "              metrics=[\"binary_crossentropy\", \"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtndLhx0uMb5",
        "colab_type": "code",
        "outputId": "b53ccc51-9f5d-4557-cacb-ee38f3c3b1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpoint_callback = ModelCheckpoint('/content/drive/My Drive/projet IA/models_crnn_weights.best.h5', monitor='val_acc', verbose=1,\n",
        "                                          save_best_only=True, mode='max')\n",
        "    \n",
        "reducelr_callback = ReduceLROnPlateau(\n",
        "                monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "callbacks_list = [checkpoint_callback, reducelr_callback]\n",
        "\"\"\"\n",
        "history = model.fit(X_train, y_enc_train, \n",
        "                      epochs=25, \n",
        "                      batch_size=100, \n",
        "                      verbose=1, \n",
        "                      validation_data=(X_test, y_enc_test)) \"\"\"\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=100, epochs=25,\n",
        "                        validation_data=(X_test, y_test), verbose=1, callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "checkpoint_callback = ModelCheckpoint('./models/crnn/weights.best.h5', monitor='val_acc', verbose=1,\n",
        "                                          save_best_only=True, mode='max')\n",
        "    \n",
        "    reducelr_callback = ReduceLROnPlateau(\n",
        "                monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\n",
        "                verbose=1\n",
        "            )\n",
        "    callbacks_list = [checkpoint_callback, reducelr_callback]\n",
        "\n",
        "    # Fit the model and get training history.\n",
        "    print('Training...')\n",
        "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT,\n",
        "                        validation_data=(x_val, y_val), verbose=1, callbacks=callbacks_list)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 6400 samples, validate on 1600 samples\n",
            "Epoch 1/25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "6400/6400 [==============================] - 10s 2ms/step - loss: 0.3656 - binary_crossentropy: 0.3656 - acc: 0.8753 - val_loss: 0.3369 - val_binary_crossentropy: 0.3369 - val_acc: 0.8749\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.87492, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 2/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.3259 - binary_crossentropy: 0.3259 - acc: 0.8788 - val_loss: 0.2828 - val_binary_crossentropy: 0.2828 - val_acc: 0.8915\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.87492 to 0.89148, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 3/25\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2959 - binary_crossentropy: 0.2959 - acc: 0.8877 - val_loss: 0.2653 - val_binary_crossentropy: 0.2653 - val_acc: 0.8991\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.89148 to 0.89914, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 4/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.2816 - binary_crossentropy: 0.2816 - acc: 0.8926 - val_loss: 0.2548 - val_binary_crossentropy: 0.2548 - val_acc: 0.9025\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89914 to 0.90250, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 5/25\n",
            "6400/6400 [==============================] - 0s 49us/step - loss: 0.2714 - binary_crossentropy: 0.2714 - acc: 0.8971 - val_loss: 0.2506 - val_binary_crossentropy: 0.2506 - val_acc: 0.9058\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.90250 to 0.90578, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 6/25\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2635 - binary_crossentropy: 0.2635 - acc: 0.8994 - val_loss: 0.2480 - val_binary_crossentropy: 0.2480 - val_acc: 0.9059\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.90578 to 0.90594, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 7/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.2549 - binary_crossentropy: 0.2549 - acc: 0.9030 - val_loss: 0.2453 - val_binary_crossentropy: 0.2453 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.90594 to 0.90852, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 8/25\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.2474 - binary_crossentropy: 0.2474 - acc: 0.9051 - val_loss: 0.2415 - val_binary_crossentropy: 0.2415 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.90852\n",
            "Epoch 9/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.2427 - binary_crossentropy: 0.2427 - acc: 0.9078 - val_loss: 0.2424 - val_binary_crossentropy: 0.2424 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.90852 to 0.90922, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 10/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.2382 - binary_crossentropy: 0.2382 - acc: 0.9101 - val_loss: 0.2418 - val_binary_crossentropy: 0.2418 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90922\n",
            "Epoch 11/25\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2348 - binary_crossentropy: 0.2348 - acc: 0.9110 - val_loss: 0.2407 - val_binary_crossentropy: 0.2407 - val_acc: 0.9093\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.90922 to 0.90930, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 12/25\n",
            "6400/6400 [==============================] - 0s 50us/step - loss: 0.2296 - binary_crossentropy: 0.2296 - acc: 0.9129 - val_loss: 0.2394 - val_binary_crossentropy: 0.2394 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.90930 to 0.90984, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 13/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.2239 - binary_crossentropy: 0.2239 - acc: 0.9157 - val_loss: 0.2405 - val_binary_crossentropy: 0.2405 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90984\n",
            "Epoch 14/25\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2166 - binary_crossentropy: 0.2166 - acc: 0.9188 - val_loss: 0.2413 - val_binary_crossentropy: 0.2413 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90984\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
            "Epoch 15/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.2086 - binary_crossentropy: 0.2086 - acc: 0.9213 - val_loss: 0.2407 - val_binary_crossentropy: 0.2407 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.90984\n",
            "Epoch 16/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.2041 - binary_crossentropy: 0.2041 - acc: 0.9236 - val_loss: 0.2432 - val_binary_crossentropy: 0.2432 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.90984\n",
            "Epoch 17/25\n",
            "6400/6400 [==============================] - 0s 62us/step - loss: 0.2016 - binary_crossentropy: 0.2016 - acc: 0.9239 - val_loss: 0.2412 - val_binary_crossentropy: 0.2412 - val_acc: 0.9105\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.90984 to 0.91055, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 18/25\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.2009 - binary_crossentropy: 0.2009 - acc: 0.9252 - val_loss: 0.2423 - val_binary_crossentropy: 0.2423 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.91055 to 0.91070, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 19/25\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.1990 - binary_crossentropy: 0.1990 - acc: 0.9247 - val_loss: 0.2422 - val_binary_crossentropy: 0.2422 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.91070 to 0.91086, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 20/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.1921 - binary_crossentropy: 0.1921 - acc: 0.9274 - val_loss: 0.2448 - val_binary_crossentropy: 0.2448 - val_acc: 0.9116\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.91086 to 0.91164, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "Epoch 21/25\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1951 - binary_crossentropy: 0.1951 - acc: 0.9272 - val_loss: 0.2433 - val_binary_crossentropy: 0.2433 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.91164\n",
            "Epoch 22/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.1924 - binary_crossentropy: 0.1924 - acc: 0.9277 - val_loss: 0.2428 - val_binary_crossentropy: 0.2428 - val_acc: 0.9110\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.91164\n",
            "Epoch 23/25\n",
            "6400/6400 [==============================] - 0s 50us/step - loss: 0.1890 - binary_crossentropy: 0.1890 - acc: 0.9295 - val_loss: 0.2458 - val_binary_crossentropy: 0.2458 - val_acc: 0.9114\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.91164\n",
            "Epoch 24/25\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.1873 - binary_crossentropy: 0.1873 - acc: 0.9294 - val_loss: 0.2466 - val_binary_crossentropy: 0.2466 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.91164 to 0.91234, saving model to /content/drive/My Drive/projet IA/models_crnn_weights.best.h5\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 25/25\n",
            "6400/6400 [==============================] - 0s 51us/step - loss: 0.1834 - binary_crossentropy: 0.1834 - acc: 0.9320 - val_loss: 0.2468 - val_binary_crossentropy: 0.2468 - val_acc: 0.9121\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91234\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncheckpoint_callback = ModelCheckpoint('./models/crnn/weights.best.h5', monitor='val_acc', verbose=1,\\n                                          save_best_only=True, mode='max')\\n    \\n    reducelr_callback = ReduceLROnPlateau(\\n                monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\\n                verbose=1\\n            )\\n    callbacks_list = [checkpoint_callback, reducelr_callback]\\n\\n    # Fit the model and get training history.\\n    print('Training...')\\n    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT,\\n                        validation_data=(x_val, y_val), verbose=1, callbacks=callbacks_list)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoA48GfxsbFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DgJ8yMZw1oP",
        "colab_type": "code",
        "outputId": "1b9cf685-7115-4d14-ca49-60a238fd5ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95be4db860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KsORetmuUcl",
        "colab_type": "code",
        "outputId": "0c87582e-334b-4a6c-e8d7-404c6396518d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "score = model.evaluate(\n",
        "\tx=X_test,\n",
        "\ty=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600/1600 [==============================] - 0s 44us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ8RfWAkuime",
        "colab_type": "code",
        "outputId": "27b42f31-1eb1-41da-bde8-89ed6fd008d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.24675972729921342\n",
            "Test accuracy: 0.24675972729921342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6pVm_9fxwwJ",
        "colab_type": "text"
      },
      "source": [
        "# Nouveau modele 2_ model performant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE_XPc2IzVjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top1(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "\n",
        "def top2(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
        "\n",
        "def top3(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
        "\n",
        "def top4(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=4)\n",
        "\n",
        "class TimeHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "        self.times = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.top1 = []\n",
        "        self.top2 = []\n",
        "        self.top3 = []\n",
        "        self.top4 = []\n",
        "        self.val_top1 = []\n",
        "        self.val_top2 = []\n",
        "        self.val_top3 = []\n",
        "        self.val_top4 = []\n",
        "        \n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.val_top1.append(logs.get('val_top1'))\n",
        "        self.val_top2.append(logs.get('val_top2'))\n",
        "        self.val_top3.append(logs.get('val_top3'))\n",
        "        self.val_top4.append(logs.get('val_top4'))\n",
        "    \n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.top1.append(logs.get('top1'))\n",
        "        self.top2.append(logs.get('top2'))\n",
        "        self.top3.append(logs.get('top3'))\n",
        "        self.top4.append(logs.get('top4'))\n",
        "        \n",
        "    def convert_to_dict(self):\n",
        "        return {\n",
        "            \"time\" : self.times,\n",
        "            \"loss\" : self.losses,\n",
        "            \"val_loss\" : self.val_losses,\n",
        "            \"top1\" : self.top1,\n",
        "            \"val_top1\" : self.val_top1,\n",
        "            \"top2\" : self.top2,\n",
        "            \"val_top2\" : self.val_top2,\n",
        "            \"top3\" : self.top3,\n",
        "            \"val_top3\" : self.val_top3,\n",
        "            \"top4\" : self.top4,\n",
        "            \"val_top4\" : self.val_top4,\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egzDEss-zO5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = pd.read_csv(\"/content/drive/My Drive/projet IA/classes.csv\", index_col = 0)\n",
        "\n",
        "#A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.\n",
        "#Methods\n",
        "#\tfit() >> Fit label binarizer\n",
        "# fit_transform() >> Fit label binarizer and transform multi-class labels to binary labels.\n",
        "enc = LabelBinarizer()\n",
        "_enc = enc.fit(label.title.values.reshape(-1, 1))\n",
        "import pickle\n",
        "pickle.dump(_enc, open(r'/content/drive/My Drive/projet IA/_saved_enc','wb'))\n",
        "y_enc = _enc.transform(label.title.values.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rf8BlIHx1HY",
        "colab_type": "code",
        "outputId": "a7f614ec-e09d-475e-8b72-cc685ca66a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "X = label.index.values\n",
        "scaler = MinMaxScaler()\n",
        "_features = pd.read_csv(\"/content/drive/My Drive/projet IA/features.csv\", header=[0, 1, 2], index_col=0)\n",
        "#This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
        "_scaler = scaler.fit(_features[_features.columns])\n",
        "\n",
        "import pickle\n",
        "pickle.dump(_scaler, open(r'/content/drive/My Drive/projet IA/_saved_scaler','wb'))\n",
        "\n",
        "_features[_features.columns] = _scaler.transform(_features[_features.columns])\n",
        "_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>feature</th>\n",
              "      <th colspan=\"40\" halign=\"left\">chroma_cens</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"33\" halign=\"left\">tonnetz</th>\n",
              "      <th colspan=\"7\" halign=\"left\">zcr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>statistics</th>\n",
              "      <th colspan=\"12\" halign=\"left\">kurtosis</th>\n",
              "      <th colspan=\"12\" halign=\"left\">max</th>\n",
              "      <th colspan=\"12\" halign=\"left\">mean</th>\n",
              "      <th colspan=\"4\" halign=\"left\">median</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"3\" halign=\"left\">max</th>\n",
              "      <th colspan=\"6\" halign=\"left\">mean</th>\n",
              "      <th colspan=\"6\" halign=\"left\">median</th>\n",
              "      <th colspan=\"6\" halign=\"left\">min</th>\n",
              "      <th colspan=\"6\" halign=\"left\">skew</th>\n",
              "      <th colspan=\"6\" halign=\"left\">std</th>\n",
              "      <th>kurtosis</th>\n",
              "      <th>max</th>\n",
              "      <th>mean</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>skew</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>...</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>track_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.201184</td>\n",
              "      <td>0.086361</td>\n",
              "      <td>0.003998</td>\n",
              "      <td>0.110705</td>\n",
              "      <td>0.072226</td>\n",
              "      <td>0.045608</td>\n",
              "      <td>0.031678</td>\n",
              "      <td>0.023291</td>\n",
              "      <td>0.034981</td>\n",
              "      <td>0.040287</td>\n",
              "      <td>0.007078</td>\n",
              "      <td>0.027610</td>\n",
              "      <td>0.662554</td>\n",
              "      <td>0.724207</td>\n",
              "      <td>0.714233</td>\n",
              "      <td>0.775197</td>\n",
              "      <td>0.693137</td>\n",
              "      <td>0.430209</td>\n",
              "      <td>0.615014</td>\n",
              "      <td>0.597478</td>\n",
              "      <td>0.683726</td>\n",
              "      <td>0.690902</td>\n",
              "      <td>0.781117</td>\n",
              "      <td>0.527419</td>\n",
              "      <td>0.670074</td>\n",
              "      <td>0.579016</td>\n",
              "      <td>0.363223</td>\n",
              "      <td>0.354682</td>\n",
              "      <td>0.340965</td>\n",
              "      <td>0.343927</td>\n",
              "      <td>0.401675</td>\n",
              "      <td>0.396190</td>\n",
              "      <td>0.318411</td>\n",
              "      <td>0.308452</td>\n",
              "      <td>0.331869</td>\n",
              "      <td>0.483411</td>\n",
              "      <td>0.682658</td>\n",
              "      <td>0.587581</td>\n",
              "      <td>0.385622</td>\n",
              "      <td>0.383078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.500288</td>\n",
              "      <td>0.204845</td>\n",
              "      <td>0.172364</td>\n",
              "      <td>0.535046</td>\n",
              "      <td>0.415169</td>\n",
              "      <td>0.544640</td>\n",
              "      <td>0.581128</td>\n",
              "      <td>0.488541</td>\n",
              "      <td>0.522006</td>\n",
              "      <td>0.544945</td>\n",
              "      <td>0.418513</td>\n",
              "      <td>0.569747</td>\n",
              "      <td>0.549825</td>\n",
              "      <td>0.527960</td>\n",
              "      <td>0.582163</td>\n",
              "      <td>0.888796</td>\n",
              "      <td>0.816496</td>\n",
              "      <td>0.629660</td>\n",
              "      <td>0.613536</td>\n",
              "      <td>0.829654</td>\n",
              "      <td>0.651703</td>\n",
              "      <td>0.605013</td>\n",
              "      <td>0.473811</td>\n",
              "      <td>0.532669</td>\n",
              "      <td>0.660159</td>\n",
              "      <td>0.538423</td>\n",
              "      <td>0.452732</td>\n",
              "      <td>0.125626</td>\n",
              "      <td>0.154102</td>\n",
              "      <td>0.087726</td>\n",
              "      <td>0.149775</td>\n",
              "      <td>0.114793</td>\n",
              "      <td>0.083786</td>\n",
              "      <td>0.004786</td>\n",
              "      <td>0.441414</td>\n",
              "      <td>0.131176</td>\n",
              "      <td>0.095114</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.133702</td>\n",
              "      <td>0.186725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.052194</td>\n",
              "      <td>0.021311</td>\n",
              "      <td>0.002979</td>\n",
              "      <td>0.087133</td>\n",
              "      <td>0.082298</td>\n",
              "      <td>0.052693</td>\n",
              "      <td>0.008314</td>\n",
              "      <td>0.004295</td>\n",
              "      <td>0.032357</td>\n",
              "      <td>0.036389</td>\n",
              "      <td>0.004641</td>\n",
              "      <td>0.026593</td>\n",
              "      <td>0.561218</td>\n",
              "      <td>0.854801</td>\n",
              "      <td>0.514090</td>\n",
              "      <td>0.436891</td>\n",
              "      <td>0.520741</td>\n",
              "      <td>0.449324</td>\n",
              "      <td>0.628908</td>\n",
              "      <td>0.696250</td>\n",
              "      <td>0.865683</td>\n",
              "      <td>0.750105</td>\n",
              "      <td>0.659008</td>\n",
              "      <td>0.547244</td>\n",
              "      <td>0.362549</td>\n",
              "      <td>0.474654</td>\n",
              "      <td>0.385783</td>\n",
              "      <td>0.338450</td>\n",
              "      <td>0.377007</td>\n",
              "      <td>0.367801</td>\n",
              "      <td>0.483530</td>\n",
              "      <td>0.468720</td>\n",
              "      <td>0.569610</td>\n",
              "      <td>0.513727</td>\n",
              "      <td>0.407613</td>\n",
              "      <td>0.331277</td>\n",
              "      <td>0.361208</td>\n",
              "      <td>0.444977</td>\n",
              "      <td>0.379695</td>\n",
              "      <td>0.357149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302009</td>\n",
              "      <td>0.264401</td>\n",
              "      <td>0.194022</td>\n",
              "      <td>0.506378</td>\n",
              "      <td>0.260443</td>\n",
              "      <td>0.476971</td>\n",
              "      <td>0.510889</td>\n",
              "      <td>0.452416</td>\n",
              "      <td>0.406653</td>\n",
              "      <td>0.525225</td>\n",
              "      <td>0.265629</td>\n",
              "      <td>0.532260</td>\n",
              "      <td>0.492674</td>\n",
              "      <td>0.490206</td>\n",
              "      <td>0.457133</td>\n",
              "      <td>0.715570</td>\n",
              "      <td>0.750463</td>\n",
              "      <td>0.289354</td>\n",
              "      <td>0.571249</td>\n",
              "      <td>0.871051</td>\n",
              "      <td>0.421172</td>\n",
              "      <td>0.629716</td>\n",
              "      <td>0.470837</td>\n",
              "      <td>0.425458</td>\n",
              "      <td>0.601850</td>\n",
              "      <td>0.535608</td>\n",
              "      <td>0.491716</td>\n",
              "      <td>0.207057</td>\n",
              "      <td>0.082439</td>\n",
              "      <td>0.261720</td>\n",
              "      <td>0.099172</td>\n",
              "      <td>0.123703</td>\n",
              "      <td>0.124554</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>0.354040</td>\n",
              "      <td>0.079249</td>\n",
              "      <td>0.055375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136344</td>\n",
              "      <td>0.134086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.123288</td>\n",
              "      <td>0.018694</td>\n",
              "      <td>0.007750</td>\n",
              "      <td>0.054350</td>\n",
              "      <td>0.069679</td>\n",
              "      <td>0.055077</td>\n",
              "      <td>0.034487</td>\n",
              "      <td>0.017586</td>\n",
              "      <td>0.030452</td>\n",
              "      <td>0.048542</td>\n",
              "      <td>0.013809</td>\n",
              "      <td>0.028177</td>\n",
              "      <td>0.375704</td>\n",
              "      <td>0.678199</td>\n",
              "      <td>0.420729</td>\n",
              "      <td>0.816637</td>\n",
              "      <td>0.736064</td>\n",
              "      <td>0.561835</td>\n",
              "      <td>0.862084</td>\n",
              "      <td>0.628425</td>\n",
              "      <td>0.565449</td>\n",
              "      <td>0.500049</td>\n",
              "      <td>0.814417</td>\n",
              "      <td>0.589480</td>\n",
              "      <td>0.321897</td>\n",
              "      <td>0.448878</td>\n",
              "      <td>0.369361</td>\n",
              "      <td>0.352504</td>\n",
              "      <td>0.293166</td>\n",
              "      <td>0.453460</td>\n",
              "      <td>0.725572</td>\n",
              "      <td>0.557252</td>\n",
              "      <td>0.439222</td>\n",
              "      <td>0.429687</td>\n",
              "      <td>0.445061</td>\n",
              "      <td>0.355585</td>\n",
              "      <td>0.325977</td>\n",
              "      <td>0.424755</td>\n",
              "      <td>0.362366</td>\n",
              "      <td>0.363819</td>\n",
              "      <td>...</td>\n",
              "      <td>0.320008</td>\n",
              "      <td>0.478809</td>\n",
              "      <td>0.145724</td>\n",
              "      <td>0.424108</td>\n",
              "      <td>0.261081</td>\n",
              "      <td>0.521781</td>\n",
              "      <td>0.368300</td>\n",
              "      <td>0.486119</td>\n",
              "      <td>0.472773</td>\n",
              "      <td>0.449974</td>\n",
              "      <td>0.264946</td>\n",
              "      <td>0.549805</td>\n",
              "      <td>0.371372</td>\n",
              "      <td>0.507924</td>\n",
              "      <td>0.521452</td>\n",
              "      <td>0.767324</td>\n",
              "      <td>0.613316</td>\n",
              "      <td>0.457599</td>\n",
              "      <td>0.288287</td>\n",
              "      <td>0.814061</td>\n",
              "      <td>0.892775</td>\n",
              "      <td>0.557296</td>\n",
              "      <td>0.437610</td>\n",
              "      <td>0.500696</td>\n",
              "      <td>0.498267</td>\n",
              "      <td>0.712932</td>\n",
              "      <td>0.583412</td>\n",
              "      <td>0.148369</td>\n",
              "      <td>0.174600</td>\n",
              "      <td>0.273816</td>\n",
              "      <td>0.226211</td>\n",
              "      <td>0.224506</td>\n",
              "      <td>0.111660</td>\n",
              "      <td>0.014725</td>\n",
              "      <td>0.433838</td>\n",
              "      <td>0.118217</td>\n",
              "      <td>0.095765</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170807</td>\n",
              "      <td>0.121202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.052329</td>\n",
              "      <td>0.014617</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.024192</td>\n",
              "      <td>0.022486</td>\n",
              "      <td>0.033369</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.006106</td>\n",
              "      <td>0.012030</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.003374</td>\n",
              "      <td>0.019829</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>0.835836</td>\n",
              "      <td>0.947928</td>\n",
              "      <td>0.792573</td>\n",
              "      <td>0.813042</td>\n",
              "      <td>0.931209</td>\n",
              "      <td>0.716524</td>\n",
              "      <td>0.878053</td>\n",
              "      <td>0.856562</td>\n",
              "      <td>0.909329</td>\n",
              "      <td>0.920701</td>\n",
              "      <td>0.579357</td>\n",
              "      <td>0.224006</td>\n",
              "      <td>0.426553</td>\n",
              "      <td>0.455488</td>\n",
              "      <td>0.401675</td>\n",
              "      <td>0.400826</td>\n",
              "      <td>0.395328</td>\n",
              "      <td>0.338418</td>\n",
              "      <td>0.304977</td>\n",
              "      <td>0.477932</td>\n",
              "      <td>0.567953</td>\n",
              "      <td>0.328225</td>\n",
              "      <td>0.177147</td>\n",
              "      <td>0.180309</td>\n",
              "      <td>0.383142</td>\n",
              "      <td>0.416721</td>\n",
              "      <td>0.417027</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651589</td>\n",
              "      <td>0.352819</td>\n",
              "      <td>0.172165</td>\n",
              "      <td>0.580406</td>\n",
              "      <td>0.303553</td>\n",
              "      <td>0.358176</td>\n",
              "      <td>0.480190</td>\n",
              "      <td>0.364900</td>\n",
              "      <td>0.415577</td>\n",
              "      <td>0.593572</td>\n",
              "      <td>0.321330</td>\n",
              "      <td>0.419688</td>\n",
              "      <td>0.479188</td>\n",
              "      <td>0.399484</td>\n",
              "      <td>0.474010</td>\n",
              "      <td>0.641831</td>\n",
              "      <td>0.712114</td>\n",
              "      <td>0.104950</td>\n",
              "      <td>0.046829</td>\n",
              "      <td>0.580241</td>\n",
              "      <td>0.564550</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.425380</td>\n",
              "      <td>0.503665</td>\n",
              "      <td>0.578529</td>\n",
              "      <td>0.563896</td>\n",
              "      <td>0.523804</td>\n",
              "      <td>0.375422</td>\n",
              "      <td>0.219861</td>\n",
              "      <td>0.364652</td>\n",
              "      <td>0.541000</td>\n",
              "      <td>0.418372</td>\n",
              "      <td>0.296749</td>\n",
              "      <td>0.008143</td>\n",
              "      <td>0.358586</td>\n",
              "      <td>0.078075</td>\n",
              "      <td>0.048860</td>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.160630</td>\n",
              "      <td>0.174871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0.044252</td>\n",
              "      <td>0.018773</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.024842</td>\n",
              "      <td>0.023775</td>\n",
              "      <td>0.020474</td>\n",
              "      <td>0.015293</td>\n",
              "      <td>0.009287</td>\n",
              "      <td>0.015200</td>\n",
              "      <td>0.012894</td>\n",
              "      <td>0.001718</td>\n",
              "      <td>0.006768</td>\n",
              "      <td>0.631437</td>\n",
              "      <td>0.803434</td>\n",
              "      <td>0.884789</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.879591</td>\n",
              "      <td>0.889782</td>\n",
              "      <td>0.859886</td>\n",
              "      <td>0.892247</td>\n",
              "      <td>0.841085</td>\n",
              "      <td>0.935253</td>\n",
              "      <td>0.909903</td>\n",
              "      <td>0.629179</td>\n",
              "      <td>0.208697</td>\n",
              "      <td>0.242774</td>\n",
              "      <td>0.334109</td>\n",
              "      <td>0.349391</td>\n",
              "      <td>0.420697</td>\n",
              "      <td>0.464945</td>\n",
              "      <td>0.455720</td>\n",
              "      <td>0.287871</td>\n",
              "      <td>0.286588</td>\n",
              "      <td>0.524940</td>\n",
              "      <td>0.549618</td>\n",
              "      <td>0.327246</td>\n",
              "      <td>0.148920</td>\n",
              "      <td>0.239877</td>\n",
              "      <td>0.329704</td>\n",
              "      <td>0.351909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.517692</td>\n",
              "      <td>0.385914</td>\n",
              "      <td>0.238340</td>\n",
              "      <td>0.613157</td>\n",
              "      <td>0.374251</td>\n",
              "      <td>0.438808</td>\n",
              "      <td>0.366924</td>\n",
              "      <td>0.455918</td>\n",
              "      <td>0.435761</td>\n",
              "      <td>0.635583</td>\n",
              "      <td>0.378768</td>\n",
              "      <td>0.501669</td>\n",
              "      <td>0.345567</td>\n",
              "      <td>0.484462</td>\n",
              "      <td>0.477299</td>\n",
              "      <td>0.566228</td>\n",
              "      <td>0.784126</td>\n",
              "      <td>0.118802</td>\n",
              "      <td>0.125306</td>\n",
              "      <td>0.764376</td>\n",
              "      <td>0.627914</td>\n",
              "      <td>0.546778</td>\n",
              "      <td>0.456223</td>\n",
              "      <td>0.471682</td>\n",
              "      <td>0.618700</td>\n",
              "      <td>0.580384</td>\n",
              "      <td>0.570716</td>\n",
              "      <td>0.414892</td>\n",
              "      <td>0.194303</td>\n",
              "      <td>0.551385</td>\n",
              "      <td>0.496838</td>\n",
              "      <td>0.346947</td>\n",
              "      <td>0.391657</td>\n",
              "      <td>0.022055</td>\n",
              "      <td>0.395960</td>\n",
              "      <td>0.058733</td>\n",
              "      <td>0.046254</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.187712</td>\n",
              "      <td>0.082691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154308</th>\n",
              "      <td>0.025204</td>\n",
              "      <td>0.012084</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>0.069297</td>\n",
              "      <td>0.102596</td>\n",
              "      <td>0.011887</td>\n",
              "      <td>0.006002</td>\n",
              "      <td>0.003335</td>\n",
              "      <td>0.012903</td>\n",
              "      <td>0.005140</td>\n",
              "      <td>0.000769</td>\n",
              "      <td>0.014556</td>\n",
              "      <td>0.589324</td>\n",
              "      <td>0.934654</td>\n",
              "      <td>0.553544</td>\n",
              "      <td>0.450362</td>\n",
              "      <td>0.891669</td>\n",
              "      <td>0.771278</td>\n",
              "      <td>0.924887</td>\n",
              "      <td>0.726307</td>\n",
              "      <td>0.806155</td>\n",
              "      <td>0.973091</td>\n",
              "      <td>0.612853</td>\n",
              "      <td>0.321465</td>\n",
              "      <td>0.302272</td>\n",
              "      <td>0.465904</td>\n",
              "      <td>0.281162</td>\n",
              "      <td>0.158276</td>\n",
              "      <td>0.201448</td>\n",
              "      <td>0.393724</td>\n",
              "      <td>0.513094</td>\n",
              "      <td>0.387963</td>\n",
              "      <td>0.544311</td>\n",
              "      <td>0.564863</td>\n",
              "      <td>0.358273</td>\n",
              "      <td>0.126056</td>\n",
              "      <td>0.292906</td>\n",
              "      <td>0.393327</td>\n",
              "      <td>0.279189</td>\n",
              "      <td>0.127462</td>\n",
              "      <td>...</td>\n",
              "      <td>0.578173</td>\n",
              "      <td>0.284063</td>\n",
              "      <td>0.185176</td>\n",
              "      <td>0.303225</td>\n",
              "      <td>0.316923</td>\n",
              "      <td>0.242936</td>\n",
              "      <td>0.466626</td>\n",
              "      <td>0.456764</td>\n",
              "      <td>0.510551</td>\n",
              "      <td>0.368837</td>\n",
              "      <td>0.316556</td>\n",
              "      <td>0.339982</td>\n",
              "      <td>0.467579</td>\n",
              "      <td>0.490776</td>\n",
              "      <td>0.564523</td>\n",
              "      <td>0.578514</td>\n",
              "      <td>0.644979</td>\n",
              "      <td>0.148042</td>\n",
              "      <td>0.196936</td>\n",
              "      <td>0.754954</td>\n",
              "      <td>0.716341</td>\n",
              "      <td>0.529115</td>\n",
              "      <td>0.520371</td>\n",
              "      <td>0.444572</td>\n",
              "      <td>0.579581</td>\n",
              "      <td>0.551142</td>\n",
              "      <td>0.530098</td>\n",
              "      <td>0.407590</td>\n",
              "      <td>0.250498</td>\n",
              "      <td>0.364031</td>\n",
              "      <td>0.388388</td>\n",
              "      <td>0.300265</td>\n",
              "      <td>0.236003</td>\n",
              "      <td>0.035252</td>\n",
              "      <td>0.761616</td>\n",
              "      <td>0.063093</td>\n",
              "      <td>0.030619</td>\n",
              "      <td>0.011429</td>\n",
              "      <td>0.257308</td>\n",
              "      <td>0.259237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154309</th>\n",
              "      <td>0.027278</td>\n",
              "      <td>0.014480</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.047213</td>\n",
              "      <td>0.009381</td>\n",
              "      <td>0.029608</td>\n",
              "      <td>0.011075</td>\n",
              "      <td>0.006157</td>\n",
              "      <td>0.007569</td>\n",
              "      <td>0.094460</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.014952</td>\n",
              "      <td>0.593390</td>\n",
              "      <td>0.566277</td>\n",
              "      <td>0.848422</td>\n",
              "      <td>0.657975</td>\n",
              "      <td>0.937840</td>\n",
              "      <td>0.710970</td>\n",
              "      <td>0.917284</td>\n",
              "      <td>0.804824</td>\n",
              "      <td>0.474034</td>\n",
              "      <td>0.854606</td>\n",
              "      <td>0.532766</td>\n",
              "      <td>0.673688</td>\n",
              "      <td>0.248449</td>\n",
              "      <td>0.253349</td>\n",
              "      <td>0.392217</td>\n",
              "      <td>0.499948</td>\n",
              "      <td>0.431585</td>\n",
              "      <td>0.467544</td>\n",
              "      <td>0.584267</td>\n",
              "      <td>0.498167</td>\n",
              "      <td>0.271992</td>\n",
              "      <td>0.143505</td>\n",
              "      <td>0.238216</td>\n",
              "      <td>0.316189</td>\n",
              "      <td>0.189088</td>\n",
              "      <td>0.247532</td>\n",
              "      <td>0.387858</td>\n",
              "      <td>0.564672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.567127</td>\n",
              "      <td>0.220878</td>\n",
              "      <td>0.190722</td>\n",
              "      <td>0.533704</td>\n",
              "      <td>0.390544</td>\n",
              "      <td>0.715603</td>\n",
              "      <td>0.412978</td>\n",
              "      <td>0.370408</td>\n",
              "      <td>0.427611</td>\n",
              "      <td>0.559476</td>\n",
              "      <td>0.387605</td>\n",
              "      <td>0.690799</td>\n",
              "      <td>0.460299</td>\n",
              "      <td>0.420726</td>\n",
              "      <td>0.451651</td>\n",
              "      <td>0.574577</td>\n",
              "      <td>0.826752</td>\n",
              "      <td>0.592804</td>\n",
              "      <td>0.131192</td>\n",
              "      <td>0.683104</td>\n",
              "      <td>0.796079</td>\n",
              "      <td>0.473802</td>\n",
              "      <td>0.487498</td>\n",
              "      <td>0.640444</td>\n",
              "      <td>0.559174</td>\n",
              "      <td>0.517525</td>\n",
              "      <td>0.601838</td>\n",
              "      <td>0.337550</td>\n",
              "      <td>0.306542</td>\n",
              "      <td>0.351917</td>\n",
              "      <td>0.626806</td>\n",
              "      <td>0.441998</td>\n",
              "      <td>0.263276</td>\n",
              "      <td>0.009122</td>\n",
              "      <td>0.761111</td>\n",
              "      <td>0.126943</td>\n",
              "      <td>0.049511</td>\n",
              "      <td>0.051429</td>\n",
              "      <td>0.168225</td>\n",
              "      <td>0.402619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154413</th>\n",
              "      <td>0.035576</td>\n",
              "      <td>0.008409</td>\n",
              "      <td>0.004902</td>\n",
              "      <td>0.049577</td>\n",
              "      <td>0.036147</td>\n",
              "      <td>0.032819</td>\n",
              "      <td>0.004404</td>\n",
              "      <td>0.008610</td>\n",
              "      <td>0.005752</td>\n",
              "      <td>0.038848</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.015059</td>\n",
              "      <td>0.890727</td>\n",
              "      <td>0.900089</td>\n",
              "      <td>0.925795</td>\n",
              "      <td>0.882089</td>\n",
              "      <td>0.965319</td>\n",
              "      <td>0.997101</td>\n",
              "      <td>0.887139</td>\n",
              "      <td>0.931908</td>\n",
              "      <td>0.823493</td>\n",
              "      <td>0.917302</td>\n",
              "      <td>0.772336</td>\n",
              "      <td>0.623201</td>\n",
              "      <td>0.608181</td>\n",
              "      <td>0.506984</td>\n",
              "      <td>0.300859</td>\n",
              "      <td>0.262693</td>\n",
              "      <td>0.281499</td>\n",
              "      <td>0.289768</td>\n",
              "      <td>0.355027</td>\n",
              "      <td>0.287962</td>\n",
              "      <td>0.293159</td>\n",
              "      <td>0.238221</td>\n",
              "      <td>0.189492</td>\n",
              "      <td>0.230614</td>\n",
              "      <td>0.573725</td>\n",
              "      <td>0.531533</td>\n",
              "      <td>0.251001</td>\n",
              "      <td>0.235541</td>\n",
              "      <td>...</td>\n",
              "      <td>0.936202</td>\n",
              "      <td>0.687639</td>\n",
              "      <td>0.605954</td>\n",
              "      <td>0.427262</td>\n",
              "      <td>0.514568</td>\n",
              "      <td>0.435786</td>\n",
              "      <td>0.649810</td>\n",
              "      <td>0.535708</td>\n",
              "      <td>0.647801</td>\n",
              "      <td>0.448034</td>\n",
              "      <td>0.502870</td>\n",
              "      <td>0.497867</td>\n",
              "      <td>0.613655</td>\n",
              "      <td>0.562835</td>\n",
              "      <td>0.689938</td>\n",
              "      <td>0.586605</td>\n",
              "      <td>0.629934</td>\n",
              "      <td>0.118216</td>\n",
              "      <td>0.193942</td>\n",
              "      <td>0.529746</td>\n",
              "      <td>0.594347</td>\n",
              "      <td>0.577217</td>\n",
              "      <td>0.487378</td>\n",
              "      <td>0.507984</td>\n",
              "      <td>0.580957</td>\n",
              "      <td>0.633106</td>\n",
              "      <td>0.593019</td>\n",
              "      <td>0.504840</td>\n",
              "      <td>0.451920</td>\n",
              "      <td>0.595939</td>\n",
              "      <td>0.536857</td>\n",
              "      <td>0.588044</td>\n",
              "      <td>0.484042</td>\n",
              "      <td>0.017220</td>\n",
              "      <td>0.300505</td>\n",
              "      <td>0.033604</td>\n",
              "      <td>0.024104</td>\n",
              "      <td>0.005714</td>\n",
              "      <td>0.175771</td>\n",
              "      <td>0.067319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154414</th>\n",
              "      <td>0.029466</td>\n",
              "      <td>0.010942</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>0.047198</td>\n",
              "      <td>0.014249</td>\n",
              "      <td>0.018654</td>\n",
              "      <td>0.014775</td>\n",
              "      <td>0.014433</td>\n",
              "      <td>0.017083</td>\n",
              "      <td>0.032635</td>\n",
              "      <td>0.002516</td>\n",
              "      <td>0.012333</td>\n",
              "      <td>0.663315</td>\n",
              "      <td>0.874991</td>\n",
              "      <td>0.889923</td>\n",
              "      <td>0.801180</td>\n",
              "      <td>0.939737</td>\n",
              "      <td>0.677490</td>\n",
              "      <td>0.907837</td>\n",
              "      <td>0.908130</td>\n",
              "      <td>0.831391</td>\n",
              "      <td>0.922067</td>\n",
              "      <td>0.810103</td>\n",
              "      <td>0.683229</td>\n",
              "      <td>0.294629</td>\n",
              "      <td>0.300571</td>\n",
              "      <td>0.282232</td>\n",
              "      <td>0.549011</td>\n",
              "      <td>0.606292</td>\n",
              "      <td>0.432152</td>\n",
              "      <td>0.255899</td>\n",
              "      <td>0.250725</td>\n",
              "      <td>0.324433</td>\n",
              "      <td>0.302895</td>\n",
              "      <td>0.321138</td>\n",
              "      <td>0.373878</td>\n",
              "      <td>0.267764</td>\n",
              "      <td>0.270914</td>\n",
              "      <td>0.202472</td>\n",
              "      <td>0.611769</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700023</td>\n",
              "      <td>0.348385</td>\n",
              "      <td>0.191096</td>\n",
              "      <td>0.437550</td>\n",
              "      <td>0.260200</td>\n",
              "      <td>0.601660</td>\n",
              "      <td>0.633525</td>\n",
              "      <td>0.453273</td>\n",
              "      <td>0.371644</td>\n",
              "      <td>0.488816</td>\n",
              "      <td>0.271575</td>\n",
              "      <td>0.637257</td>\n",
              "      <td>0.649852</td>\n",
              "      <td>0.482066</td>\n",
              "      <td>0.430890</td>\n",
              "      <td>0.471199</td>\n",
              "      <td>0.695730</td>\n",
              "      <td>0.154999</td>\n",
              "      <td>0.166768</td>\n",
              "      <td>0.703435</td>\n",
              "      <td>0.576811</td>\n",
              "      <td>0.519167</td>\n",
              "      <td>0.452845</td>\n",
              "      <td>0.419514</td>\n",
              "      <td>0.542848</td>\n",
              "      <td>0.572106</td>\n",
              "      <td>0.542857</td>\n",
              "      <td>0.500160</td>\n",
              "      <td>0.199635</td>\n",
              "      <td>0.489228</td>\n",
              "      <td>0.630196</td>\n",
              "      <td>0.392552</td>\n",
              "      <td>0.374241</td>\n",
              "      <td>0.014625</td>\n",
              "      <td>0.494949</td>\n",
              "      <td>0.068074</td>\n",
              "      <td>0.044951</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.182423</td>\n",
              "      <td>0.136856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155066</th>\n",
              "      <td>0.041370</td>\n",
              "      <td>0.018581</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.050016</td>\n",
              "      <td>0.015390</td>\n",
              "      <td>0.010806</td>\n",
              "      <td>0.010911</td>\n",
              "      <td>0.001199</td>\n",
              "      <td>0.017194</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.540354</td>\n",
              "      <td>0.788773</td>\n",
              "      <td>0.820485</td>\n",
              "      <td>0.887561</td>\n",
              "      <td>0.832352</td>\n",
              "      <td>0.800968</td>\n",
              "      <td>0.951394</td>\n",
              "      <td>0.728863</td>\n",
              "      <td>0.929229</td>\n",
              "      <td>0.723849</td>\n",
              "      <td>0.935677</td>\n",
              "      <td>0.590297</td>\n",
              "      <td>0.189313</td>\n",
              "      <td>0.267072</td>\n",
              "      <td>0.309086</td>\n",
              "      <td>0.369014</td>\n",
              "      <td>0.316267</td>\n",
              "      <td>0.381342</td>\n",
              "      <td>0.464521</td>\n",
              "      <td>0.396699</td>\n",
              "      <td>0.378566</td>\n",
              "      <td>0.484052</td>\n",
              "      <td>0.456173</td>\n",
              "      <td>0.315343</td>\n",
              "      <td>0.169918</td>\n",
              "      <td>0.288198</td>\n",
              "      <td>0.248082</td>\n",
              "      <td>0.354589</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635171</td>\n",
              "      <td>0.353674</td>\n",
              "      <td>0.198413</td>\n",
              "      <td>0.490409</td>\n",
              "      <td>0.323390</td>\n",
              "      <td>0.541520</td>\n",
              "      <td>0.369218</td>\n",
              "      <td>0.440053</td>\n",
              "      <td>0.454432</td>\n",
              "      <td>0.512177</td>\n",
              "      <td>0.310362</td>\n",
              "      <td>0.523977</td>\n",
              "      <td>0.336037</td>\n",
              "      <td>0.474932</td>\n",
              "      <td>0.526076</td>\n",
              "      <td>0.736164</td>\n",
              "      <td>0.790579</td>\n",
              "      <td>0.375738</td>\n",
              "      <td>0.165418</td>\n",
              "      <td>0.609384</td>\n",
              "      <td>0.668820</td>\n",
              "      <td>0.568978</td>\n",
              "      <td>0.518499</td>\n",
              "      <td>0.631541</td>\n",
              "      <td>0.647083</td>\n",
              "      <td>0.554424</td>\n",
              "      <td>0.525332</td>\n",
              "      <td>0.345787</td>\n",
              "      <td>0.204270</td>\n",
              "      <td>0.456725</td>\n",
              "      <td>0.662392</td>\n",
              "      <td>0.444601</td>\n",
              "      <td>0.294599</td>\n",
              "      <td>0.074712</td>\n",
              "      <td>0.539394</td>\n",
              "      <td>0.020072</td>\n",
              "      <td>0.013029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.327822</td>\n",
              "      <td>0.089425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 518 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "feature    chroma_cens                      ...       zcr                    \n",
              "statistics    kurtosis                      ...       min      skew       std\n",
              "number              01        02        03  ...        01        01        01\n",
              "track_id                                    ...                              \n",
              "2             0.201184  0.086361  0.003998  ...  0.000000  0.133702  0.186725\n",
              "5             0.052194  0.021311  0.002979  ...  0.000000  0.136344  0.134086\n",
              "10            0.123288  0.018694  0.007750  ...  0.000000  0.170807  0.121202\n",
              "140           0.052329  0.014617  0.001425  ...  0.011429  0.160630  0.174871\n",
              "141           0.044252  0.018773  0.001264  ...  0.017143  0.187712  0.082691\n",
              "...                ...       ...       ...  ...       ...       ...       ...\n",
              "154308        0.025204  0.012084  0.002196  ...  0.011429  0.257308  0.259237\n",
              "154309        0.027278  0.014480  0.003143  ...  0.051429  0.168225  0.402619\n",
              "154413        0.035576  0.008409  0.004902  ...  0.005714  0.175771  0.067319\n",
              "154414        0.029466  0.010942  0.002972  ...  0.020000  0.182423  0.136856\n",
              "155066        0.041370  0.018581  0.003100  ...  0.000000  0.327822  0.089425\n",
              "\n",
              "[8000 rows x 518 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEUEpq2HyxdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# phase ACP\n",
        "pca = PCA(n_components=0.95)\n",
        "# Fit the model with X and apply the dimensionality reduction on X\n",
        "pc = pca.fit(_features)\n",
        "import pickle\n",
        "pickle.dump(pc, open(r'/content/drive/My Drive/projet IA/_saved_pca','wb'))\n",
        "new_X = pc.transform(_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l6uaNlty8sx",
        "colab_type": "code",
        "outputId": "6ad2eb0d-456c-4bcf-900d-7fa8c4e80cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "new_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmFyT2i6zJHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_X, y_enc, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4JsU3N6DDIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#used whene we load the model \n",
        "# X_train.shape[1] = 148\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=148, activation='relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "              optimizer='Nadam', \n",
        "              metrics=[\"binary_crossentropy\", top1, top2, top3, top4])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w7Kcko9EShO",
        "colab_type": "code",
        "outputId": "a6a2a7da-5d71-415f-881f-8a45ab010135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkAQy4KZzfhE",
        "colab_type": "code",
        "outputId": "50ed97e7-5fc3-4c36-fdb1-248e1333b69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "#the model \n",
        "\"\"\"\n",
        "model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(30, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 128)               19072     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 8)                 136       \n",
            "=================================================================\n",
            "Total params: 30,072\n",
            "Trainable params: 30,072\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn4SsYeOzqPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='Nadam', \n",
        "              metrics=[\"binary_crossentropy\", top1, top2, top3, top4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSSMQGDKzunU",
        "colab_type": "code",
        "outputId": "eb4ed1d6-de58-42fb-f9c0-259c38c8f0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6400, 148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKM1eVUPzzhz",
        "colab_type": "code",
        "outputId": "a80c8eb7-94d8-44e5-9b8e-c95fb3dfcad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath=\"/content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\", verbose=1, save_best_only=True)\n",
        "#model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose=2, callbacks=[checkpointer])\n",
        "\n",
        "#def load_trained_model(weights_path):\n",
        "#   model = create_model()\n",
        "#   model.load_weights(weights_path)\n",
        "\n",
        "# a revoire il est possible de diminuer l'epochs a 100\n",
        "time_callback = TimeHistory()\n",
        "model.fit(X_train, y_train, \n",
        "          epochs=150, \n",
        "          batch_size=100, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[checkpointer, time_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6400 samples, validate on 1600 samples\n",
            "Epoch 1/150\n",
            "6400/6400 [==============================] - 1s 129us/step - loss: 0.3735 - binary_crossentropy: 0.3735 - top1: 0.1578 - top2: 0.3000 - top3: 0.4383 - top4: 0.5731 - val_loss: 0.3576 - val_binary_crossentropy: 0.3576 - val_top1: 0.3481 - val_top2: 0.5225 - val_top3: 0.6344 - val_top4: 0.7538\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.35761, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 2/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.3530 - binary_crossentropy: 0.3530 - top1: 0.2573 - top2: 0.4441 - top3: 0.5856 - top4: 0.7155 - val_loss: 0.3254 - val_binary_crossentropy: 0.3254 - val_top1: 0.3619 - val_top2: 0.5819 - val_top3: 0.7194 - val_top4: 0.8094\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.35761 to 0.32544, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 3/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.3345 - binary_crossentropy: 0.3345 - top1: 0.3195 - top2: 0.5187 - top3: 0.6534 - top4: 0.7645 - val_loss: 0.3055 - val_binary_crossentropy: 0.3055 - val_top1: 0.4394 - val_top2: 0.6125 - val_top3: 0.7181 - val_top4: 0.8450\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.32544 to 0.30555, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 4/150\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.3239 - binary_crossentropy: 0.3239 - top1: 0.3548 - top2: 0.5617 - top3: 0.6944 - top4: 0.8036 - val_loss: 0.2966 - val_binary_crossentropy: 0.2966 - val_top1: 0.4550 - val_top2: 0.6494 - val_top3: 0.7706 - val_top4: 0.8550\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30555 to 0.29656, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 5/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.3142 - binary_crossentropy: 0.3142 - top1: 0.3952 - top2: 0.5927 - top3: 0.7177 - top4: 0.8139 - val_loss: 0.2896 - val_binary_crossentropy: 0.2896 - val_top1: 0.4812 - val_top2: 0.6738 - val_top3: 0.7894 - val_top4: 0.8706\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.29656 to 0.28957, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 6/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.3048 - binary_crossentropy: 0.3048 - top1: 0.4170 - top2: 0.6192 - top3: 0.7395 - top4: 0.8303 - val_loss: 0.2831 - val_binary_crossentropy: 0.2831 - val_top1: 0.4987 - val_top2: 0.6931 - val_top3: 0.8006 - val_top4: 0.8744\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.28957 to 0.28314, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 7/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.3002 - binary_crossentropy: 0.3002 - top1: 0.4287 - top2: 0.6356 - top3: 0.7616 - top4: 0.8453 - val_loss: 0.2751 - val_binary_crossentropy: 0.2751 - val_top1: 0.5175 - val_top2: 0.6962 - val_top3: 0.8056 - val_top4: 0.8769\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.28314 to 0.27508, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 8/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.2925 - binary_crossentropy: 0.2925 - top1: 0.4564 - top2: 0.6541 - top3: 0.7666 - top4: 0.8458 - val_loss: 0.2695 - val_binary_crossentropy: 0.2695 - val_top1: 0.5281 - val_top2: 0.7075 - val_top3: 0.8144 - val_top4: 0.8800\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.27508 to 0.26953, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 9/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.2857 - binary_crossentropy: 0.2857 - top1: 0.4781 - top2: 0.6806 - top3: 0.7873 - top4: 0.8689 - val_loss: 0.2653 - val_binary_crossentropy: 0.2653 - val_top1: 0.5475 - val_top2: 0.7119 - val_top3: 0.8219 - val_top4: 0.8838\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.26953 to 0.26530, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 10/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.2819 - binary_crossentropy: 0.2819 - top1: 0.4884 - top2: 0.6825 - top3: 0.7881 - top4: 0.8683 - val_loss: 0.2599 - val_binary_crossentropy: 0.2599 - val_top1: 0.5531 - val_top2: 0.7175 - val_top3: 0.8281 - val_top4: 0.8919\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.26530 to 0.25993, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 11/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2754 - binary_crossentropy: 0.2754 - top1: 0.5006 - top2: 0.6867 - top3: 0.7962 - top4: 0.8681 - val_loss: 0.2586 - val_binary_crossentropy: 0.2586 - val_top1: 0.5538 - val_top2: 0.7231 - val_top3: 0.8213 - val_top4: 0.8856\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.25993 to 0.25861, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 12/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2717 - binary_crossentropy: 0.2717 - top1: 0.5187 - top2: 0.7003 - top3: 0.8078 - top4: 0.8791 - val_loss: 0.2560 - val_binary_crossentropy: 0.2560 - val_top1: 0.5581 - val_top2: 0.7231 - val_top3: 0.8194 - val_top4: 0.8869\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.25861 to 0.25601, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 13/150\n",
            "6400/6400 [==============================] - 0s 72us/step - loss: 0.2664 - binary_crossentropy: 0.2664 - top1: 0.5287 - top2: 0.7163 - top3: 0.8133 - top4: 0.8828 - val_loss: 0.2541 - val_binary_crossentropy: 0.2541 - val_top1: 0.5737 - val_top2: 0.7263 - val_top3: 0.8175 - val_top4: 0.8850\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.25601 to 0.25406, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 14/150\n",
            "6400/6400 [==============================] - 0s 62us/step - loss: 0.2629 - binary_crossentropy: 0.2629 - top1: 0.5417 - top2: 0.7217 - top3: 0.8155 - top4: 0.8884 - val_loss: 0.2559 - val_binary_crossentropy: 0.2559 - val_top1: 0.5688 - val_top2: 0.7219 - val_top3: 0.8250 - val_top4: 0.8813\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.25406\n",
            "Epoch 15/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2590 - binary_crossentropy: 0.2590 - top1: 0.5508 - top2: 0.7238 - top3: 0.8217 - top4: 0.8906 - val_loss: 0.2536 - val_binary_crossentropy: 0.2536 - val_top1: 0.5637 - val_top2: 0.7237 - val_top3: 0.8175 - val_top4: 0.8763\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.25406 to 0.25359, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 16/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2540 - binary_crossentropy: 0.2540 - top1: 0.5531 - top2: 0.7352 - top3: 0.8317 - top4: 0.8997 - val_loss: 0.2537 - val_binary_crossentropy: 0.2537 - val_top1: 0.5750 - val_top2: 0.7281 - val_top3: 0.8287 - val_top4: 0.8881\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.25359\n",
            "Epoch 17/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.2505 - binary_crossentropy: 0.2505 - top1: 0.5681 - top2: 0.7475 - top3: 0.8400 - top4: 0.8987 - val_loss: 0.2510 - val_binary_crossentropy: 0.2510 - val_top1: 0.5769 - val_top2: 0.7350 - val_top3: 0.8212 - val_top4: 0.8806\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.25359 to 0.25102, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 18/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2487 - binary_crossentropy: 0.2487 - top1: 0.5737 - top2: 0.7520 - top3: 0.8422 - top4: 0.8998 - val_loss: 0.2500 - val_binary_crossentropy: 0.2500 - val_top1: 0.5837 - val_top2: 0.7313 - val_top3: 0.8231 - val_top4: 0.8844\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.25102 to 0.24999, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 19/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.2452 - binary_crossentropy: 0.2452 - top1: 0.5847 - top2: 0.7523 - top3: 0.8433 - top4: 0.9064 - val_loss: 0.2491 - val_binary_crossentropy: 0.2491 - val_top1: 0.5781 - val_top2: 0.7375 - val_top3: 0.8244 - val_top4: 0.8813\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.24999 to 0.24906, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 20/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2405 - binary_crossentropy: 0.2405 - top1: 0.5902 - top2: 0.7623 - top3: 0.8480 - top4: 0.9056 - val_loss: 0.2507 - val_binary_crossentropy: 0.2507 - val_top1: 0.5706 - val_top2: 0.7331 - val_top3: 0.8237 - val_top4: 0.8837\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.24906\n",
            "Epoch 21/150\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2387 - binary_crossentropy: 0.2387 - top1: 0.5956 - top2: 0.7653 - top3: 0.8470 - top4: 0.9091 - val_loss: 0.2501 - val_binary_crossentropy: 0.2501 - val_top1: 0.5819 - val_top2: 0.7437 - val_top3: 0.8219 - val_top4: 0.8825\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.24906\n",
            "Epoch 22/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2399 - binary_crossentropy: 0.2399 - top1: 0.6036 - top2: 0.7630 - top3: 0.8458 - top4: 0.9058 - val_loss: 0.2507 - val_binary_crossentropy: 0.2507 - val_top1: 0.5587 - val_top2: 0.7288 - val_top3: 0.8206 - val_top4: 0.8850\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.24906\n",
            "Epoch 23/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.2355 - binary_crossentropy: 0.2355 - top1: 0.5998 - top2: 0.7652 - top3: 0.8483 - top4: 0.9078 - val_loss: 0.2495 - val_binary_crossentropy: 0.2495 - val_top1: 0.5762 - val_top2: 0.7362 - val_top3: 0.8187 - val_top4: 0.8806\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.24906\n",
            "Epoch 24/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.2307 - binary_crossentropy: 0.2307 - top1: 0.6078 - top2: 0.7686 - top3: 0.8567 - top4: 0.9153 - val_loss: 0.2477 - val_binary_crossentropy: 0.2477 - val_top1: 0.5825 - val_top2: 0.7444 - val_top3: 0.8237 - val_top4: 0.8825\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.24906 to 0.24769, saving model to /content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\n",
            "Epoch 25/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.2312 - binary_crossentropy: 0.2312 - top1: 0.6120 - top2: 0.7720 - top3: 0.8569 - top4: 0.9136 - val_loss: 0.2521 - val_binary_crossentropy: 0.2521 - val_top1: 0.5756 - val_top2: 0.7312 - val_top3: 0.8138 - val_top4: 0.8838\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.24769\n",
            "Epoch 26/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.2292 - binary_crossentropy: 0.2292 - top1: 0.6189 - top2: 0.7772 - top3: 0.8628 - top4: 0.9116 - val_loss: 0.2524 - val_binary_crossentropy: 0.2524 - val_top1: 0.5831 - val_top2: 0.7400 - val_top3: 0.8256 - val_top4: 0.8881\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.24769\n",
            "Epoch 27/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2273 - binary_crossentropy: 0.2273 - top1: 0.6262 - top2: 0.7755 - top3: 0.8605 - top4: 0.9205 - val_loss: 0.2519 - val_binary_crossentropy: 0.2519 - val_top1: 0.5738 - val_top2: 0.7469 - val_top3: 0.8206 - val_top4: 0.8837\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.24769\n",
            "Epoch 28/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2195 - binary_crossentropy: 0.2195 - top1: 0.6344 - top2: 0.7898 - top3: 0.8664 - top4: 0.9198 - val_loss: 0.2566 - val_binary_crossentropy: 0.2566 - val_top1: 0.5806 - val_top2: 0.7325 - val_top3: 0.8137 - val_top4: 0.8831\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.24769\n",
            "Epoch 29/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.2180 - binary_crossentropy: 0.2180 - top1: 0.6356 - top2: 0.7908 - top3: 0.8711 - top4: 0.9219 - val_loss: 0.2549 - val_binary_crossentropy: 0.2549 - val_top1: 0.5825 - val_top2: 0.7387 - val_top3: 0.8269 - val_top4: 0.8837\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.24769\n",
            "Epoch 30/150\n",
            "6400/6400 [==============================] - 0s 64us/step - loss: 0.2186 - binary_crossentropy: 0.2186 - top1: 0.6439 - top2: 0.7944 - top3: 0.8673 - top4: 0.9178 - val_loss: 0.2541 - val_binary_crossentropy: 0.2541 - val_top1: 0.5825 - val_top2: 0.7350 - val_top3: 0.8219 - val_top4: 0.8837\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.24769\n",
            "Epoch 31/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.2143 - binary_crossentropy: 0.2143 - top1: 0.6486 - top2: 0.7992 - top3: 0.8780 - top4: 0.9255 - val_loss: 0.2555 - val_binary_crossentropy: 0.2555 - val_top1: 0.5769 - val_top2: 0.7350 - val_top3: 0.8181 - val_top4: 0.8831\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.24769\n",
            "Epoch 32/150\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.2170 - binary_crossentropy: 0.2170 - top1: 0.6386 - top2: 0.7923 - top3: 0.8689 - top4: 0.9175 - val_loss: 0.2547 - val_binary_crossentropy: 0.2547 - val_top1: 0.5775 - val_top2: 0.7356 - val_top3: 0.8219 - val_top4: 0.8837\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.24769\n",
            "Epoch 33/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.2134 - binary_crossentropy: 0.2134 - top1: 0.6530 - top2: 0.8022 - top3: 0.8755 - top4: 0.9258 - val_loss: 0.2552 - val_binary_crossentropy: 0.2552 - val_top1: 0.5781 - val_top2: 0.7350 - val_top3: 0.8162 - val_top4: 0.8850\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.24769\n",
            "Epoch 34/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2086 - binary_crossentropy: 0.2086 - top1: 0.6567 - top2: 0.8022 - top3: 0.8767 - top4: 0.9220 - val_loss: 0.2577 - val_binary_crossentropy: 0.2577 - val_top1: 0.5706 - val_top2: 0.7325 - val_top3: 0.8106 - val_top4: 0.8794\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.24769\n",
            "Epoch 35/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.2153 - binary_crossentropy: 0.2153 - top1: 0.6539 - top2: 0.7995 - top3: 0.8759 - top4: 0.9223 - val_loss: 0.2582 - val_binary_crossentropy: 0.2582 - val_top1: 0.5700 - val_top2: 0.7319 - val_top3: 0.8056 - val_top4: 0.8719\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.24769\n",
            "Epoch 36/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.2087 - binary_crossentropy: 0.2087 - top1: 0.6538 - top2: 0.8067 - top3: 0.8781 - top4: 0.9259 - val_loss: 0.2597 - val_binary_crossentropy: 0.2597 - val_top1: 0.5706 - val_top2: 0.7331 - val_top3: 0.8162 - val_top4: 0.8894\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.24769\n",
            "Epoch 37/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2047 - binary_crossentropy: 0.2047 - top1: 0.6692 - top2: 0.8086 - top3: 0.8766 - top4: 0.9298 - val_loss: 0.2657 - val_binary_crossentropy: 0.2657 - val_top1: 0.5606 - val_top2: 0.7250 - val_top3: 0.8069 - val_top4: 0.8769\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.24769\n",
            "Epoch 38/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.2075 - binary_crossentropy: 0.2075 - top1: 0.6625 - top2: 0.8092 - top3: 0.8802 - top4: 0.9238 - val_loss: 0.2609 - val_binary_crossentropy: 0.2609 - val_top1: 0.5712 - val_top2: 0.7219 - val_top3: 0.8075 - val_top4: 0.8800\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.24769\n",
            "Epoch 39/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.2030 - binary_crossentropy: 0.2030 - top1: 0.6744 - top2: 0.8164 - top3: 0.8875 - top4: 0.9325 - val_loss: 0.2637 - val_binary_crossentropy: 0.2637 - val_top1: 0.5600 - val_top2: 0.7175 - val_top3: 0.8012 - val_top4: 0.8788\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.24769\n",
            "Epoch 40/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.2013 - binary_crossentropy: 0.2013 - top1: 0.6678 - top2: 0.8178 - top3: 0.8852 - top4: 0.9292 - val_loss: 0.2610 - val_binary_crossentropy: 0.2610 - val_top1: 0.5637 - val_top2: 0.7138 - val_top3: 0.8081 - val_top4: 0.8800\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.24769\n",
            "Epoch 41/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.2006 - binary_crossentropy: 0.2006 - top1: 0.6767 - top2: 0.8178 - top3: 0.8859 - top4: 0.9333 - val_loss: 0.2648 - val_binary_crossentropy: 0.2648 - val_top1: 0.5687 - val_top2: 0.7156 - val_top3: 0.8006 - val_top4: 0.8750\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.24769\n",
            "Epoch 42/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1967 - binary_crossentropy: 0.1967 - top1: 0.6888 - top2: 0.8191 - top3: 0.8870 - top4: 0.9331 - val_loss: 0.2676 - val_binary_crossentropy: 0.2676 - val_top1: 0.5644 - val_top2: 0.7206 - val_top3: 0.8156 - val_top4: 0.8812\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.24769\n",
            "Epoch 43/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1950 - binary_crossentropy: 0.1950 - top1: 0.6830 - top2: 0.8177 - top3: 0.8880 - top4: 0.9320 - val_loss: 0.2692 - val_binary_crossentropy: 0.2692 - val_top1: 0.5656 - val_top2: 0.7244 - val_top3: 0.8062 - val_top4: 0.8725\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.24769\n",
            "Epoch 44/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1950 - binary_crossentropy: 0.1950 - top1: 0.6908 - top2: 0.8253 - top3: 0.8888 - top4: 0.9302 - val_loss: 0.2696 - val_binary_crossentropy: 0.2696 - val_top1: 0.5625 - val_top2: 0.7144 - val_top3: 0.8056 - val_top4: 0.8744\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.24769\n",
            "Epoch 45/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1957 - binary_crossentropy: 0.1957 - top1: 0.6909 - top2: 0.8206 - top3: 0.8864 - top4: 0.9306 - val_loss: 0.2690 - val_binary_crossentropy: 0.2690 - val_top1: 0.5619 - val_top2: 0.7088 - val_top3: 0.8031 - val_top4: 0.8750\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.24769\n",
            "Epoch 46/150\n",
            "6400/6400 [==============================] - 0s 62us/step - loss: 0.1937 - binary_crossentropy: 0.1937 - top1: 0.6880 - top2: 0.8189 - top3: 0.8856 - top4: 0.9311 - val_loss: 0.2683 - val_binary_crossentropy: 0.2683 - val_top1: 0.5569 - val_top2: 0.7175 - val_top3: 0.8056 - val_top4: 0.8669\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.24769\n",
            "Epoch 47/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1945 - binary_crossentropy: 0.1945 - top1: 0.6930 - top2: 0.8252 - top3: 0.8877 - top4: 0.9297 - val_loss: 0.2733 - val_binary_crossentropy: 0.2733 - val_top1: 0.5506 - val_top2: 0.7119 - val_top3: 0.7969 - val_top4: 0.8650\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.24769\n",
            "Epoch 48/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1886 - binary_crossentropy: 0.1886 - top1: 0.7020 - top2: 0.8348 - top3: 0.8970 - top4: 0.9367 - val_loss: 0.2716 - val_binary_crossentropy: 0.2716 - val_top1: 0.5662 - val_top2: 0.7200 - val_top3: 0.8069 - val_top4: 0.8706\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.24769\n",
            "Epoch 49/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1894 - binary_crossentropy: 0.1894 - top1: 0.7031 - top2: 0.8381 - top3: 0.9006 - top4: 0.9395 - val_loss: 0.2758 - val_binary_crossentropy: 0.2758 - val_top1: 0.5637 - val_top2: 0.7194 - val_top3: 0.8025 - val_top4: 0.8719\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.24769\n",
            "Epoch 50/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1878 - binary_crossentropy: 0.1878 - top1: 0.7059 - top2: 0.8325 - top3: 0.8966 - top4: 0.9367 - val_loss: 0.2765 - val_binary_crossentropy: 0.2765 - val_top1: 0.5800 - val_top2: 0.7144 - val_top3: 0.8031 - val_top4: 0.8712\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.24769\n",
            "Epoch 51/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1920 - binary_crossentropy: 0.1920 - top1: 0.7053 - top2: 0.8364 - top3: 0.8992 - top4: 0.9359 - val_loss: 0.2777 - val_binary_crossentropy: 0.2777 - val_top1: 0.5594 - val_top2: 0.7069 - val_top3: 0.7919 - val_top4: 0.8625\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.24769\n",
            "Epoch 52/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1880 - binary_crossentropy: 0.1880 - top1: 0.7077 - top2: 0.8339 - top3: 0.8939 - top4: 0.9333 - val_loss: 0.2747 - val_binary_crossentropy: 0.2747 - val_top1: 0.5638 - val_top2: 0.7138 - val_top3: 0.8000 - val_top4: 0.8681\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.24769\n",
            "Epoch 53/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1875 - binary_crossentropy: 0.1875 - top1: 0.7048 - top2: 0.8370 - top3: 0.8961 - top4: 0.9353 - val_loss: 0.2807 - val_binary_crossentropy: 0.2807 - val_top1: 0.5512 - val_top2: 0.7119 - val_top3: 0.7956 - val_top4: 0.8669\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.24769\n",
            "Epoch 54/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1871 - binary_crossentropy: 0.1871 - top1: 0.7098 - top2: 0.8344 - top3: 0.8964 - top4: 0.9359 - val_loss: 0.2778 - val_binary_crossentropy: 0.2778 - val_top1: 0.5587 - val_top2: 0.7131 - val_top3: 0.8000 - val_top4: 0.8687\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.24769\n",
            "Epoch 55/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1834 - binary_crossentropy: 0.1834 - top1: 0.7120 - top2: 0.8377 - top3: 0.8975 - top4: 0.9377 - val_loss: 0.2794 - val_binary_crossentropy: 0.2794 - val_top1: 0.5506 - val_top2: 0.7106 - val_top3: 0.8044 - val_top4: 0.8706\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.24769\n",
            "Epoch 56/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1838 - binary_crossentropy: 0.1838 - top1: 0.7159 - top2: 0.8431 - top3: 0.9011 - top4: 0.9391 - val_loss: 0.2795 - val_binary_crossentropy: 0.2795 - val_top1: 0.5569 - val_top2: 0.7100 - val_top3: 0.8037 - val_top4: 0.8738\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.24769\n",
            "Epoch 57/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1819 - binary_crossentropy: 0.1819 - top1: 0.7166 - top2: 0.8416 - top3: 0.8966 - top4: 0.9359 - val_loss: 0.2870 - val_binary_crossentropy: 0.2870 - val_top1: 0.5581 - val_top2: 0.7088 - val_top3: 0.7962 - val_top4: 0.8713\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.24769\n",
            "Epoch 58/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1814 - binary_crossentropy: 0.1814 - top1: 0.7177 - top2: 0.8441 - top3: 0.9022 - top4: 0.9425 - val_loss: 0.2879 - val_binary_crossentropy: 0.2879 - val_top1: 0.5537 - val_top2: 0.7075 - val_top3: 0.7950 - val_top4: 0.8706\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.24769\n",
            "Epoch 59/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1830 - binary_crossentropy: 0.1830 - top1: 0.7173 - top2: 0.8330 - top3: 0.8994 - top4: 0.9391 - val_loss: 0.2841 - val_binary_crossentropy: 0.2841 - val_top1: 0.5581 - val_top2: 0.7044 - val_top3: 0.7981 - val_top4: 0.8706\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.24769\n",
            "Epoch 60/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1825 - binary_crossentropy: 0.1825 - top1: 0.7127 - top2: 0.8398 - top3: 0.8997 - top4: 0.9403 - val_loss: 0.2884 - val_binary_crossentropy: 0.2884 - val_top1: 0.5469 - val_top2: 0.7094 - val_top3: 0.7944 - val_top4: 0.8669\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.24769\n",
            "Epoch 61/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1834 - binary_crossentropy: 0.1834 - top1: 0.7170 - top2: 0.8406 - top3: 0.8991 - top4: 0.9387 - val_loss: 0.2922 - val_binary_crossentropy: 0.2922 - val_top1: 0.5487 - val_top2: 0.7044 - val_top3: 0.7975 - val_top4: 0.8619\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.24769\n",
            "Epoch 62/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1806 - binary_crossentropy: 0.1806 - top1: 0.7211 - top2: 0.8392 - top3: 0.8995 - top4: 0.9375 - val_loss: 0.2905 - val_binary_crossentropy: 0.2905 - val_top1: 0.5575 - val_top2: 0.7031 - val_top3: 0.7925 - val_top4: 0.8625\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.24769\n",
            "Epoch 63/150\n",
            "6400/6400 [==============================] - 0s 53us/step - loss: 0.1770 - binary_crossentropy: 0.1770 - top1: 0.7222 - top2: 0.8487 - top3: 0.9020 - top4: 0.9412 - val_loss: 0.2924 - val_binary_crossentropy: 0.2924 - val_top1: 0.5525 - val_top2: 0.7112 - val_top3: 0.7994 - val_top4: 0.8675\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.24769\n",
            "Epoch 64/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1719 - binary_crossentropy: 0.1719 - top1: 0.7338 - top2: 0.8550 - top3: 0.9123 - top4: 0.9467 - val_loss: 0.2933 - val_binary_crossentropy: 0.2933 - val_top1: 0.5669 - val_top2: 0.7181 - val_top3: 0.8044 - val_top4: 0.8675\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.24769\n",
            "Epoch 65/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1806 - binary_crossentropy: 0.1806 - top1: 0.7186 - top2: 0.8400 - top3: 0.8994 - top4: 0.9375 - val_loss: 0.2831 - val_binary_crossentropy: 0.2831 - val_top1: 0.5544 - val_top2: 0.7106 - val_top3: 0.8025 - val_top4: 0.8675\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.24769\n",
            "Epoch 66/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1750 - binary_crossentropy: 0.1750 - top1: 0.7266 - top2: 0.8469 - top3: 0.9027 - top4: 0.9422 - val_loss: 0.2921 - val_binary_crossentropy: 0.2921 - val_top1: 0.5644 - val_top2: 0.7119 - val_top3: 0.7975 - val_top4: 0.8656\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.24769\n",
            "Epoch 67/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1751 - binary_crossentropy: 0.1751 - top1: 0.7336 - top2: 0.8502 - top3: 0.9086 - top4: 0.9425 - val_loss: 0.2955 - val_binary_crossentropy: 0.2955 - val_top1: 0.5619 - val_top2: 0.7131 - val_top3: 0.8006 - val_top4: 0.8606\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.24769\n",
            "Epoch 68/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1788 - binary_crossentropy: 0.1788 - top1: 0.7252 - top2: 0.8444 - top3: 0.9053 - top4: 0.9411 - val_loss: 0.2901 - val_binary_crossentropy: 0.2901 - val_top1: 0.5637 - val_top2: 0.7119 - val_top3: 0.8062 - val_top4: 0.8656\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.24769\n",
            "Epoch 69/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1762 - binary_crossentropy: 0.1762 - top1: 0.7258 - top2: 0.8430 - top3: 0.9008 - top4: 0.9439 - val_loss: 0.2878 - val_binary_crossentropy: 0.2878 - val_top1: 0.5625 - val_top2: 0.7094 - val_top3: 0.7931 - val_top4: 0.8656\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.24769\n",
            "Epoch 70/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1753 - binary_crossentropy: 0.1753 - top1: 0.7266 - top2: 0.8477 - top3: 0.9036 - top4: 0.9420 - val_loss: 0.2963 - val_binary_crossentropy: 0.2963 - val_top1: 0.5569 - val_top2: 0.6975 - val_top3: 0.7875 - val_top4: 0.8619\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.24769\n",
            "Epoch 71/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1733 - binary_crossentropy: 0.1733 - top1: 0.7331 - top2: 0.8494 - top3: 0.9056 - top4: 0.9442 - val_loss: 0.2989 - val_binary_crossentropy: 0.2989 - val_top1: 0.5637 - val_top2: 0.7050 - val_top3: 0.7944 - val_top4: 0.8619\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.24769\n",
            "Epoch 72/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1735 - binary_crossentropy: 0.1735 - top1: 0.7323 - top2: 0.8506 - top3: 0.9069 - top4: 0.9441 - val_loss: 0.2936 - val_binary_crossentropy: 0.2936 - val_top1: 0.5537 - val_top2: 0.7063 - val_top3: 0.7963 - val_top4: 0.8588\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.24769\n",
            "Epoch 73/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1691 - binary_crossentropy: 0.1691 - top1: 0.7409 - top2: 0.8556 - top3: 0.9131 - top4: 0.9433 - val_loss: 0.2943 - val_binary_crossentropy: 0.2943 - val_top1: 0.5662 - val_top2: 0.7075 - val_top3: 0.7956 - val_top4: 0.8631\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.24769\n",
            "Epoch 74/150\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.1710 - binary_crossentropy: 0.1710 - top1: 0.7348 - top2: 0.8511 - top3: 0.9053 - top4: 0.9417 - val_loss: 0.2986 - val_binary_crossentropy: 0.2986 - val_top1: 0.5650 - val_top2: 0.7069 - val_top3: 0.7887 - val_top4: 0.8594\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.24769\n",
            "Epoch 75/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1744 - binary_crossentropy: 0.1744 - top1: 0.7319 - top2: 0.8489 - top3: 0.9102 - top4: 0.9453 - val_loss: 0.2920 - val_binary_crossentropy: 0.2920 - val_top1: 0.5525 - val_top2: 0.6956 - val_top3: 0.7844 - val_top4: 0.8588\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.24769\n",
            "Epoch 76/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1652 - binary_crossentropy: 0.1652 - top1: 0.7436 - top2: 0.8617 - top3: 0.9156 - top4: 0.9466 - val_loss: 0.3008 - val_binary_crossentropy: 0.3008 - val_top1: 0.5562 - val_top2: 0.7063 - val_top3: 0.7969 - val_top4: 0.8681\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.24769\n",
            "Epoch 77/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1660 - binary_crossentropy: 0.1660 - top1: 0.7514 - top2: 0.8606 - top3: 0.9128 - top4: 0.9456 - val_loss: 0.3015 - val_binary_crossentropy: 0.3015 - val_top1: 0.5562 - val_top2: 0.7025 - val_top3: 0.7875 - val_top4: 0.8619\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.24769\n",
            "Epoch 78/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1702 - binary_crossentropy: 0.1702 - top1: 0.7361 - top2: 0.8489 - top3: 0.9028 - top4: 0.9394 - val_loss: 0.2959 - val_binary_crossentropy: 0.2959 - val_top1: 0.5587 - val_top2: 0.6938 - val_top3: 0.7881 - val_top4: 0.8656\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.24769\n",
            "Epoch 79/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1701 - binary_crossentropy: 0.1701 - top1: 0.7370 - top2: 0.8516 - top3: 0.9094 - top4: 0.9447 - val_loss: 0.2991 - val_binary_crossentropy: 0.2991 - val_top1: 0.5550 - val_top2: 0.6944 - val_top3: 0.7869 - val_top4: 0.8500\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.24769\n",
            "Epoch 80/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1663 - binary_crossentropy: 0.1663 - top1: 0.7436 - top2: 0.8573 - top3: 0.9111 - top4: 0.9436 - val_loss: 0.2994 - val_binary_crossentropy: 0.2994 - val_top1: 0.5662 - val_top2: 0.6988 - val_top3: 0.7925 - val_top4: 0.8575\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.24769\n",
            "Epoch 81/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1659 - binary_crossentropy: 0.1659 - top1: 0.7478 - top2: 0.8606 - top3: 0.9166 - top4: 0.9488 - val_loss: 0.3041 - val_binary_crossentropy: 0.3041 - val_top1: 0.5575 - val_top2: 0.7044 - val_top3: 0.7969 - val_top4: 0.8663\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.24769\n",
            "Epoch 82/150\n",
            "6400/6400 [==============================] - 0s 62us/step - loss: 0.1658 - binary_crossentropy: 0.1658 - top1: 0.7484 - top2: 0.8625 - top3: 0.9144 - top4: 0.9470 - val_loss: 0.3019 - val_binary_crossentropy: 0.3019 - val_top1: 0.5544 - val_top2: 0.6963 - val_top3: 0.7906 - val_top4: 0.8606\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.24769\n",
            "Epoch 83/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1659 - binary_crossentropy: 0.1659 - top1: 0.7430 - top2: 0.8544 - top3: 0.9123 - top4: 0.9477 - val_loss: 0.3080 - val_binary_crossentropy: 0.3080 - val_top1: 0.5606 - val_top2: 0.7006 - val_top3: 0.7931 - val_top4: 0.8575\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.24769\n",
            "Epoch 84/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1626 - binary_crossentropy: 0.1626 - top1: 0.7516 - top2: 0.8616 - top3: 0.9161 - top4: 0.9473 - val_loss: 0.3076 - val_binary_crossentropy: 0.3076 - val_top1: 0.5625 - val_top2: 0.7069 - val_top3: 0.7956 - val_top4: 0.8650\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.24769\n",
            "Epoch 85/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1619 - binary_crossentropy: 0.1619 - top1: 0.7541 - top2: 0.8638 - top3: 0.9164 - top4: 0.9491 - val_loss: 0.3091 - val_binary_crossentropy: 0.3091 - val_top1: 0.5569 - val_top2: 0.6925 - val_top3: 0.7812 - val_top4: 0.8594\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.24769\n",
            "Epoch 86/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1643 - binary_crossentropy: 0.1643 - top1: 0.7472 - top2: 0.8614 - top3: 0.9141 - top4: 0.9466 - val_loss: 0.3094 - val_binary_crossentropy: 0.3094 - val_top1: 0.5431 - val_top2: 0.6938 - val_top3: 0.7825 - val_top4: 0.8544\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.24769\n",
            "Epoch 87/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1655 - binary_crossentropy: 0.1655 - top1: 0.7450 - top2: 0.8642 - top3: 0.9108 - top4: 0.9442 - val_loss: 0.3102 - val_binary_crossentropy: 0.3102 - val_top1: 0.5506 - val_top2: 0.6900 - val_top3: 0.7825 - val_top4: 0.8506\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.24769\n",
            "Epoch 88/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1668 - binary_crossentropy: 0.1668 - top1: 0.7480 - top2: 0.8606 - top3: 0.9102 - top4: 0.9403 - val_loss: 0.3146 - val_binary_crossentropy: 0.3146 - val_top1: 0.5481 - val_top2: 0.6975 - val_top3: 0.7863 - val_top4: 0.8537\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.24769\n",
            "Epoch 89/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1625 - binary_crossentropy: 0.1625 - top1: 0.7461 - top2: 0.8620 - top3: 0.9139 - top4: 0.9498 - val_loss: 0.3162 - val_binary_crossentropy: 0.3162 - val_top1: 0.5600 - val_top2: 0.6944 - val_top3: 0.7781 - val_top4: 0.8494\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.24769\n",
            "Epoch 90/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1588 - binary_crossentropy: 0.1588 - top1: 0.7563 - top2: 0.8648 - top3: 0.9189 - top4: 0.9514 - val_loss: 0.3260 - val_binary_crossentropy: 0.3260 - val_top1: 0.5563 - val_top2: 0.6956 - val_top3: 0.7837 - val_top4: 0.8519\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.24769\n",
            "Epoch 91/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1636 - binary_crossentropy: 0.1636 - top1: 0.7488 - top2: 0.8644 - top3: 0.9150 - top4: 0.9464 - val_loss: 0.3168 - val_binary_crossentropy: 0.3168 - val_top1: 0.5562 - val_top2: 0.6969 - val_top3: 0.7800 - val_top4: 0.8469\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.24769\n",
            "Epoch 92/150\n",
            "6400/6400 [==============================] - 0s 76us/step - loss: 0.1649 - binary_crossentropy: 0.1649 - top1: 0.7445 - top2: 0.8611 - top3: 0.9153 - top4: 0.9461 - val_loss: 0.3126 - val_binary_crossentropy: 0.3126 - val_top1: 0.5550 - val_top2: 0.6937 - val_top3: 0.7875 - val_top4: 0.8531\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.24769\n",
            "Epoch 93/150\n",
            "6400/6400 [==============================] - 0s 67us/step - loss: 0.1623 - binary_crossentropy: 0.1623 - top1: 0.7480 - top2: 0.8597 - top3: 0.9139 - top4: 0.9481 - val_loss: 0.3132 - val_binary_crossentropy: 0.3132 - val_top1: 0.5550 - val_top2: 0.6994 - val_top3: 0.7887 - val_top4: 0.8544\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.24769\n",
            "Epoch 94/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1629 - binary_crossentropy: 0.1629 - top1: 0.7612 - top2: 0.8691 - top3: 0.9172 - top4: 0.9486 - val_loss: 0.3178 - val_binary_crossentropy: 0.3178 - val_top1: 0.5544 - val_top2: 0.6913 - val_top3: 0.7819 - val_top4: 0.8550\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.24769\n",
            "Epoch 95/150\n",
            "6400/6400 [==============================] - 0s 52us/step - loss: 0.1619 - binary_crossentropy: 0.1619 - top1: 0.7573 - top2: 0.8662 - top3: 0.9134 - top4: 0.9475 - val_loss: 0.3241 - val_binary_crossentropy: 0.3241 - val_top1: 0.5519 - val_top2: 0.6863 - val_top3: 0.7775 - val_top4: 0.8537\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.24769\n",
            "Epoch 96/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1623 - binary_crossentropy: 0.1623 - top1: 0.7552 - top2: 0.8675 - top3: 0.9164 - top4: 0.9502 - val_loss: 0.3235 - val_binary_crossentropy: 0.3235 - val_top1: 0.5581 - val_top2: 0.6881 - val_top3: 0.7775 - val_top4: 0.8456\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.24769\n",
            "Epoch 97/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1611 - binary_crossentropy: 0.1611 - top1: 0.7552 - top2: 0.8666 - top3: 0.9156 - top4: 0.9448 - val_loss: 0.3273 - val_binary_crossentropy: 0.3273 - val_top1: 0.5512 - val_top2: 0.6900 - val_top3: 0.7731 - val_top4: 0.8456\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.24769\n",
            "Epoch 98/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1619 - binary_crossentropy: 0.1619 - top1: 0.7539 - top2: 0.8658 - top3: 0.9205 - top4: 0.9508 - val_loss: 0.3216 - val_binary_crossentropy: 0.3216 - val_top1: 0.5594 - val_top2: 0.6888 - val_top3: 0.7744 - val_top4: 0.8544\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.24769\n",
            "Epoch 99/150\n",
            "6400/6400 [==============================] - 0s 57us/step - loss: 0.1588 - binary_crossentropy: 0.1588 - top1: 0.7597 - top2: 0.8650 - top3: 0.9164 - top4: 0.9491 - val_loss: 0.3269 - val_binary_crossentropy: 0.3269 - val_top1: 0.5500 - val_top2: 0.6863 - val_top3: 0.7750 - val_top4: 0.8506\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.24769\n",
            "Epoch 100/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1587 - binary_crossentropy: 0.1587 - top1: 0.7616 - top2: 0.8673 - top3: 0.9161 - top4: 0.9473 - val_loss: 0.3230 - val_binary_crossentropy: 0.3230 - val_top1: 0.5519 - val_top2: 0.6900 - val_top3: 0.7781 - val_top4: 0.8506\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.24769\n",
            "Epoch 101/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1570 - binary_crossentropy: 0.1570 - top1: 0.7617 - top2: 0.8709 - top3: 0.9184 - top4: 0.9520 - val_loss: 0.3280 - val_binary_crossentropy: 0.3280 - val_top1: 0.5562 - val_top2: 0.6894 - val_top3: 0.7937 - val_top4: 0.8631\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.24769\n",
            "Epoch 102/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1585 - binary_crossentropy: 0.1585 - top1: 0.7689 - top2: 0.8648 - top3: 0.9183 - top4: 0.9497 - val_loss: 0.3269 - val_binary_crossentropy: 0.3269 - val_top1: 0.5638 - val_top2: 0.6969 - val_top3: 0.7837 - val_top4: 0.8581\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.24769\n",
            "Epoch 103/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1589 - binary_crossentropy: 0.1589 - top1: 0.7617 - top2: 0.8656 - top3: 0.9169 - top4: 0.9492 - val_loss: 0.3267 - val_binary_crossentropy: 0.3267 - val_top1: 0.5619 - val_top2: 0.6913 - val_top3: 0.7912 - val_top4: 0.8575\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.24769\n",
            "Epoch 104/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1568 - binary_crossentropy: 0.1568 - top1: 0.7633 - top2: 0.8695 - top3: 0.9139 - top4: 0.9463 - val_loss: 0.3247 - val_binary_crossentropy: 0.3247 - val_top1: 0.5594 - val_top2: 0.6950 - val_top3: 0.7881 - val_top4: 0.8581\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.24769\n",
            "Epoch 105/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1603 - binary_crossentropy: 0.1603 - top1: 0.7548 - top2: 0.8664 - top3: 0.9158 - top4: 0.9448 - val_loss: 0.3251 - val_binary_crossentropy: 0.3251 - val_top1: 0.5544 - val_top2: 0.6900 - val_top3: 0.7838 - val_top4: 0.8563\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.24769\n",
            "Epoch 106/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1610 - binary_crossentropy: 0.1610 - top1: 0.7567 - top2: 0.8664 - top3: 0.9163 - top4: 0.9483 - val_loss: 0.3248 - val_binary_crossentropy: 0.3248 - val_top1: 0.5687 - val_top2: 0.7006 - val_top3: 0.7906 - val_top4: 0.8569\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.24769\n",
            "Epoch 107/150\n",
            "6400/6400 [==============================] - 0s 56us/step - loss: 0.1574 - binary_crossentropy: 0.1574 - top1: 0.7605 - top2: 0.8683 - top3: 0.9163 - top4: 0.9494 - val_loss: 0.3260 - val_binary_crossentropy: 0.3260 - val_top1: 0.5656 - val_top2: 0.7006 - val_top3: 0.7881 - val_top4: 0.8569\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.24769\n",
            "Epoch 108/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1598 - binary_crossentropy: 0.1598 - top1: 0.7642 - top2: 0.8684 - top3: 0.9197 - top4: 0.9520 - val_loss: 0.3202 - val_binary_crossentropy: 0.3202 - val_top1: 0.5612 - val_top2: 0.6919 - val_top3: 0.7856 - val_top4: 0.8537\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.24769\n",
            "Epoch 109/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1550 - binary_crossentropy: 0.1550 - top1: 0.7664 - top2: 0.8731 - top3: 0.9219 - top4: 0.9527 - val_loss: 0.3298 - val_binary_crossentropy: 0.3298 - val_top1: 0.5569 - val_top2: 0.6994 - val_top3: 0.7806 - val_top4: 0.8513\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.24769\n",
            "Epoch 110/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1552 - binary_crossentropy: 0.1552 - top1: 0.7684 - top2: 0.8733 - top3: 0.9189 - top4: 0.9531 - val_loss: 0.3279 - val_binary_crossentropy: 0.3279 - val_top1: 0.5469 - val_top2: 0.6944 - val_top3: 0.7894 - val_top4: 0.8550\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.24769\n",
            "Epoch 111/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1540 - binary_crossentropy: 0.1540 - top1: 0.7722 - top2: 0.8744 - top3: 0.9238 - top4: 0.9544 - val_loss: 0.3314 - val_binary_crossentropy: 0.3314 - val_top1: 0.5513 - val_top2: 0.6888 - val_top3: 0.7819 - val_top4: 0.8487\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.24769\n",
            "Epoch 112/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1532 - binary_crossentropy: 0.1532 - top1: 0.7658 - top2: 0.8695 - top3: 0.9198 - top4: 0.9509 - val_loss: 0.3333 - val_binary_crossentropy: 0.3333 - val_top1: 0.5463 - val_top2: 0.6794 - val_top3: 0.7762 - val_top4: 0.8369\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.24769\n",
            "Epoch 113/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1528 - binary_crossentropy: 0.1528 - top1: 0.7717 - top2: 0.8744 - top3: 0.9209 - top4: 0.9519 - val_loss: 0.3327 - val_binary_crossentropy: 0.3327 - val_top1: 0.5544 - val_top2: 0.6931 - val_top3: 0.7719 - val_top4: 0.8456\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.24769\n",
            "Epoch 114/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1553 - binary_crossentropy: 0.1553 - top1: 0.7694 - top2: 0.8758 - top3: 0.9216 - top4: 0.9522 - val_loss: 0.3344 - val_binary_crossentropy: 0.3344 - val_top1: 0.5588 - val_top2: 0.6831 - val_top3: 0.7837 - val_top4: 0.8438\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.24769\n",
            "Epoch 115/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1568 - binary_crossentropy: 0.1568 - top1: 0.7658 - top2: 0.8694 - top3: 0.9200 - top4: 0.9511 - val_loss: 0.3287 - val_binary_crossentropy: 0.3287 - val_top1: 0.5619 - val_top2: 0.6931 - val_top3: 0.7881 - val_top4: 0.8563\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.24769\n",
            "Epoch 116/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1547 - binary_crossentropy: 0.1547 - top1: 0.7733 - top2: 0.8711 - top3: 0.9225 - top4: 0.9505 - val_loss: 0.3287 - val_binary_crossentropy: 0.3287 - val_top1: 0.5544 - val_top2: 0.6825 - val_top3: 0.7731 - val_top4: 0.8444\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.24769\n",
            "Epoch 117/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1539 - binary_crossentropy: 0.1539 - top1: 0.7708 - top2: 0.8748 - top3: 0.9189 - top4: 0.9477 - val_loss: 0.3374 - val_binary_crossentropy: 0.3374 - val_top1: 0.5569 - val_top2: 0.6906 - val_top3: 0.7781 - val_top4: 0.8556\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.24769\n",
            "Epoch 118/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1524 - binary_crossentropy: 0.1524 - top1: 0.7775 - top2: 0.8762 - top3: 0.9213 - top4: 0.9527 - val_loss: 0.3332 - val_binary_crossentropy: 0.3332 - val_top1: 0.5444 - val_top2: 0.6863 - val_top3: 0.7781 - val_top4: 0.8513\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.24769\n",
            "Epoch 119/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1533 - binary_crossentropy: 0.1533 - top1: 0.7730 - top2: 0.8769 - top3: 0.9264 - top4: 0.9505 - val_loss: 0.3330 - val_binary_crossentropy: 0.3330 - val_top1: 0.5575 - val_top2: 0.6825 - val_top3: 0.7806 - val_top4: 0.8569\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.24769\n",
            "Epoch 120/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1535 - binary_crossentropy: 0.1535 - top1: 0.7750 - top2: 0.8786 - top3: 0.9220 - top4: 0.9502 - val_loss: 0.3299 - val_binary_crossentropy: 0.3299 - val_top1: 0.5481 - val_top2: 0.6862 - val_top3: 0.7781 - val_top4: 0.8475\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.24769\n",
            "Epoch 121/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1500 - binary_crossentropy: 0.1500 - top1: 0.7744 - top2: 0.8770 - top3: 0.9247 - top4: 0.9531 - val_loss: 0.3322 - val_binary_crossentropy: 0.3322 - val_top1: 0.5506 - val_top2: 0.6819 - val_top3: 0.7812 - val_top4: 0.8531\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.24769\n",
            "Epoch 122/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1523 - binary_crossentropy: 0.1523 - top1: 0.7802 - top2: 0.8784 - top3: 0.9214 - top4: 0.9502 - val_loss: 0.3402 - val_binary_crossentropy: 0.3402 - val_top1: 0.5556 - val_top2: 0.6900 - val_top3: 0.7788 - val_top4: 0.8438\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.24769\n",
            "Epoch 123/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1524 - binary_crossentropy: 0.1524 - top1: 0.7728 - top2: 0.8733 - top3: 0.9184 - top4: 0.9481 - val_loss: 0.3413 - val_binary_crossentropy: 0.3413 - val_top1: 0.5437 - val_top2: 0.6856 - val_top3: 0.7819 - val_top4: 0.8512\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.24769\n",
            "Epoch 124/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1538 - binary_crossentropy: 0.1538 - top1: 0.7727 - top2: 0.8744 - top3: 0.9208 - top4: 0.9497 - val_loss: 0.3349 - val_binary_crossentropy: 0.3349 - val_top1: 0.5563 - val_top2: 0.6931 - val_top3: 0.7869 - val_top4: 0.8550\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.24769\n",
            "Epoch 125/150\n",
            "6400/6400 [==============================] - 0s 64us/step - loss: 0.1512 - binary_crossentropy: 0.1512 - top1: 0.7784 - top2: 0.8720 - top3: 0.9200 - top4: 0.9509 - val_loss: 0.3376 - val_binary_crossentropy: 0.3376 - val_top1: 0.5512 - val_top2: 0.6912 - val_top3: 0.7869 - val_top4: 0.8531\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.24769\n",
            "Epoch 126/150\n",
            "6400/6400 [==============================] - 0s 64us/step - loss: 0.1511 - binary_crossentropy: 0.1511 - top1: 0.7767 - top2: 0.8711 - top3: 0.9236 - top4: 0.9544 - val_loss: 0.3455 - val_binary_crossentropy: 0.3455 - val_top1: 0.5469 - val_top2: 0.6925 - val_top3: 0.7850 - val_top4: 0.8519\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.24769\n",
            "Epoch 127/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1509 - binary_crossentropy: 0.1509 - top1: 0.7747 - top2: 0.8772 - top3: 0.9213 - top4: 0.9495 - val_loss: 0.3368 - val_binary_crossentropy: 0.3368 - val_top1: 0.5462 - val_top2: 0.6863 - val_top3: 0.7862 - val_top4: 0.8606\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.24769\n",
            "Epoch 128/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1503 - binary_crossentropy: 0.1503 - top1: 0.7773 - top2: 0.8727 - top3: 0.9213 - top4: 0.9523 - val_loss: 0.3458 - val_binary_crossentropy: 0.3458 - val_top1: 0.5450 - val_top2: 0.6938 - val_top3: 0.7812 - val_top4: 0.8500\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.24769\n",
            "Epoch 129/150\n",
            "6400/6400 [==============================] - 0s 62us/step - loss: 0.1527 - binary_crossentropy: 0.1527 - top1: 0.7700 - top2: 0.8780 - top3: 0.9302 - top4: 0.9559 - val_loss: 0.3316 - val_binary_crossentropy: 0.3316 - val_top1: 0.5475 - val_top2: 0.6825 - val_top3: 0.7737 - val_top4: 0.8506\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.24769\n",
            "Epoch 130/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1491 - binary_crossentropy: 0.1491 - top1: 0.7795 - top2: 0.8756 - top3: 0.9213 - top4: 0.9503 - val_loss: 0.3385 - val_binary_crossentropy: 0.3385 - val_top1: 0.5575 - val_top2: 0.6869 - val_top3: 0.7744 - val_top4: 0.8494\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.24769\n",
            "Epoch 131/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1492 - binary_crossentropy: 0.1492 - top1: 0.7781 - top2: 0.8809 - top3: 0.9252 - top4: 0.9552 - val_loss: 0.3332 - val_binary_crossentropy: 0.3332 - val_top1: 0.5569 - val_top2: 0.6906 - val_top3: 0.7869 - val_top4: 0.8550\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.24769\n",
            "Epoch 132/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1491 - binary_crossentropy: 0.1491 - top1: 0.7722 - top2: 0.8770 - top3: 0.9242 - top4: 0.9530 - val_loss: 0.3429 - val_binary_crossentropy: 0.3429 - val_top1: 0.5587 - val_top2: 0.6881 - val_top3: 0.7831 - val_top4: 0.8569\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.24769\n",
            "Epoch 133/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1467 - binary_crossentropy: 0.1467 - top1: 0.7797 - top2: 0.8762 - top3: 0.9217 - top4: 0.9547 - val_loss: 0.3403 - val_binary_crossentropy: 0.3403 - val_top1: 0.5512 - val_top2: 0.6819 - val_top3: 0.7787 - val_top4: 0.8444\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.24769\n",
            "Epoch 134/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1506 - binary_crossentropy: 0.1506 - top1: 0.7767 - top2: 0.8777 - top3: 0.9214 - top4: 0.9523 - val_loss: 0.3422 - val_binary_crossentropy: 0.3422 - val_top1: 0.5425 - val_top2: 0.6862 - val_top3: 0.7794 - val_top4: 0.8500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.24769\n",
            "Epoch 135/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1520 - binary_crossentropy: 0.1520 - top1: 0.7770 - top2: 0.8786 - top3: 0.9220 - top4: 0.9486 - val_loss: 0.3423 - val_binary_crossentropy: 0.3423 - val_top1: 0.5519 - val_top2: 0.6919 - val_top3: 0.7844 - val_top4: 0.8487\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.24769\n",
            "Epoch 136/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1493 - binary_crossentropy: 0.1493 - top1: 0.7805 - top2: 0.8789 - top3: 0.9220 - top4: 0.9522 - val_loss: 0.3360 - val_binary_crossentropy: 0.3360 - val_top1: 0.5431 - val_top2: 0.6800 - val_top3: 0.7781 - val_top4: 0.8531\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.24769\n",
            "Epoch 137/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1458 - binary_crossentropy: 0.1458 - top1: 0.7828 - top2: 0.8803 - top3: 0.9216 - top4: 0.9508 - val_loss: 0.3517 - val_binary_crossentropy: 0.3517 - val_top1: 0.5400 - val_top2: 0.6781 - val_top3: 0.7713 - val_top4: 0.8456\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.24769\n",
            "Epoch 138/150\n",
            "6400/6400 [==============================] - 0s 65us/step - loss: 0.1506 - binary_crossentropy: 0.1506 - top1: 0.7773 - top2: 0.8834 - top3: 0.9244 - top4: 0.9475 - val_loss: 0.3495 - val_binary_crossentropy: 0.3495 - val_top1: 0.5431 - val_top2: 0.6819 - val_top3: 0.7725 - val_top4: 0.8475\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.24769\n",
            "Epoch 139/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1542 - binary_crossentropy: 0.1542 - top1: 0.7762 - top2: 0.8761 - top3: 0.9238 - top4: 0.9500 - val_loss: 0.3341 - val_binary_crossentropy: 0.3341 - val_top1: 0.5444 - val_top2: 0.6850 - val_top3: 0.7800 - val_top4: 0.8519\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.24769\n",
            "Epoch 140/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1572 - binary_crossentropy: 0.1572 - top1: 0.7703 - top2: 0.8714 - top3: 0.9167 - top4: 0.9517 - val_loss: 0.3290 - val_binary_crossentropy: 0.3290 - val_top1: 0.5444 - val_top2: 0.6819 - val_top3: 0.7700 - val_top4: 0.8494\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.24769\n",
            "Epoch 141/150\n",
            "6400/6400 [==============================] - 0s 63us/step - loss: 0.1464 - binary_crossentropy: 0.1464 - top1: 0.7752 - top2: 0.8794 - top3: 0.9288 - top4: 0.9575 - val_loss: 0.3424 - val_binary_crossentropy: 0.3424 - val_top1: 0.5494 - val_top2: 0.6763 - val_top3: 0.7756 - val_top4: 0.8487\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.24769\n",
            "Epoch 142/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1444 - binary_crossentropy: 0.1444 - top1: 0.7861 - top2: 0.8828 - top3: 0.9283 - top4: 0.9563 - val_loss: 0.3395 - val_binary_crossentropy: 0.3395 - val_top1: 0.5425 - val_top2: 0.6762 - val_top3: 0.7787 - val_top4: 0.8513\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.24769\n",
            "Epoch 143/150\n",
            "6400/6400 [==============================] - 0s 64us/step - loss: 0.1472 - binary_crossentropy: 0.1472 - top1: 0.7777 - top2: 0.8817 - top3: 0.9272 - top4: 0.9541 - val_loss: 0.3427 - val_binary_crossentropy: 0.3427 - val_top1: 0.5519 - val_top2: 0.6856 - val_top3: 0.7869 - val_top4: 0.8606\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.24769\n",
            "Epoch 144/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1485 - binary_crossentropy: 0.1485 - top1: 0.7834 - top2: 0.8788 - top3: 0.9239 - top4: 0.9525 - val_loss: 0.3450 - val_binary_crossentropy: 0.3450 - val_top1: 0.5488 - val_top2: 0.6831 - val_top3: 0.7794 - val_top4: 0.8531\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.24769\n",
            "Epoch 145/150\n",
            "6400/6400 [==============================] - 0s 60us/step - loss: 0.1470 - binary_crossentropy: 0.1470 - top1: 0.7839 - top2: 0.8822 - top3: 0.9267 - top4: 0.9559 - val_loss: 0.3413 - val_binary_crossentropy: 0.3413 - val_top1: 0.5581 - val_top2: 0.6969 - val_top3: 0.7919 - val_top4: 0.8513\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.24769\n",
            "Epoch 146/150\n",
            "6400/6400 [==============================] - 0s 54us/step - loss: 0.1491 - binary_crossentropy: 0.1491 - top1: 0.7798 - top2: 0.8761 - top3: 0.9223 - top4: 0.9511 - val_loss: 0.3426 - val_binary_crossentropy: 0.3426 - val_top1: 0.5506 - val_top2: 0.6831 - val_top3: 0.7825 - val_top4: 0.8631\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.24769\n",
            "Epoch 147/150\n",
            "6400/6400 [==============================] - 0s 55us/step - loss: 0.1437 - binary_crossentropy: 0.1437 - top1: 0.7862 - top2: 0.8841 - top3: 0.9252 - top4: 0.9552 - val_loss: 0.3403 - val_binary_crossentropy: 0.3403 - val_top1: 0.5500 - val_top2: 0.6963 - val_top3: 0.7894 - val_top4: 0.8644\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.24769\n",
            "Epoch 148/150\n",
            "6400/6400 [==============================] - 0s 61us/step - loss: 0.1457 - binary_crossentropy: 0.1457 - top1: 0.7833 - top2: 0.8816 - top3: 0.9259 - top4: 0.9559 - val_loss: 0.3428 - val_binary_crossentropy: 0.3428 - val_top1: 0.5562 - val_top2: 0.6931 - val_top3: 0.7887 - val_top4: 0.8638\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.24769\n",
            "Epoch 149/150\n",
            "6400/6400 [==============================] - 0s 59us/step - loss: 0.1488 - binary_crossentropy: 0.1488 - top1: 0.7830 - top2: 0.8823 - top3: 0.9308 - top4: 0.9534 - val_loss: 0.3320 - val_binary_crossentropy: 0.3320 - val_top1: 0.5587 - val_top2: 0.6881 - val_top3: 0.7869 - val_top4: 0.8644\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.24769\n",
            "Epoch 150/150\n",
            "6400/6400 [==============================] - 0s 58us/step - loss: 0.1439 - binary_crossentropy: 0.1439 - top1: 0.7880 - top2: 0.8869 - top3: 0.9300 - top4: 0.9553 - val_loss: 0.3414 - val_binary_crossentropy: 0.3414 - val_top1: 0.5587 - val_top2: 0.6831 - val_top3: 0.7937 - val_top4: 0.8594\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.24769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95b01c6470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHKaQpYk0FEa",
        "colab_type": "code",
        "outputId": "0a9d9c42-df6e-4b8d-8f4f-5e4aba2b21d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600/1600 [==============================] - 0s 65us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.341375994682312, 0.341375994682312, 0.55875, 0.683125, 0.79375, 0.859375]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHU4ETHdBVlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "_json = model.to_json()\n",
        "# Save the json on a file\n",
        "import json\n",
        "with open('/content/drive/My Drive/projet IA/personal.json', 'w') as json_file:\n",
        "    json.dump(_json, json_file)\n",
        "model.save_weights(\"/content/drive/My Drive/projet IA/weight.h5\",save_format=\"h5\")\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDVxMfFFDP30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"with open('personal.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "__model = keras.models.model_from_json(data)\n",
        "__model.load_weights(\"/content/weight.h5\")\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA1kTsOMMjYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "#Converting predictions to label\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "#Converting one hot encoded test label to label\n",
        "test = list()\n",
        "for i in range(len(y_test)):\n",
        "    test.append(np.argmax(y_test[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udNjxw6oMiZw",
        "colab_type": "code",
        "outputId": "95c1e628-e270-4261-f459-e367c8f7295b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "a = accuracy_score(pred,test)\n",
        "print('Accuracy is:', a*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 55.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_33lbIng0OS8",
        "colab_type": "code",
        "outputId": "34c111ba-0008-460b-9848-0abb14d80e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "score = model.evaluate(\n",
        "\tx=X_test,\n",
        "\ty=y_test)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600/1600 [==============================] - 0s 54us/step\n",
            "Test loss: 0.341375994682312\n",
            "Test accuracy: 0.55875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYuy_Mf-jpO",
        "colab_type": "text"
      },
      "source": [
        "**Save & loaD** model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvBTZ2P_1PII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model saving\n",
        "#model.save('/content/drive/My Drive/projet IA/fully_connected_MLP_VF.h5')  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfRYTnTu3qCF",
        "colab_type": "code",
        "outputId": "ce0d4596-19d8-4403-fcd8-3d70d56cc34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#load le model \n",
        "\n",
        "from keras.models import load_model\n",
        "model_load = create_model()\n",
        "model_load.load_weights(\"/content/drive/My Drive/projet IA/fully_connected_MLP_VF_weights_test.hdf5\")\n",
        "\"\"\"\n",
        "with open('personal.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "model_load = keras.models.model_from_json(data)\n",
        "model_load.load_weights(\"/content/weight.h5\")\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwith open(\\'personal.json\\') as json_file:\\n    data = json.load(json_file)\\nmodel_load = keras.models.model_from_json(data)\\nmodel_load.load_weights(\"/content/weight.h5\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN_bDKsSGIG4",
        "colab_type": "code",
        "outputId": "ec1e3f18-b945-4cc3-ed8a-e98234c1129a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# evaluate the model\n",
        "model_load.evaluate(X_test,  y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.24769114583730698, 0.24769114583730698, 0.5825, 0.744375, 0.82375, 0.8825]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGs_EL7pqwrN",
        "colab_type": "text"
      },
      "source": [
        "# USE THE MODEL TO PREDICT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMGmswi5ABsU",
        "colab_type": "code",
        "outputId": "9cc3f7bc-e43e-4546-8462-e5507e1b6328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# example to use the load model\n",
        "y_pred = model_load.predict(X_train[200:201])\n",
        "print(y_pred, \"============\", y_train[200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7.8602323e-05 2.0966506e-02 2.1169140e-04 2.3223080e-05 1.2562417e-03\n",
            "  5.3444499e-05 8.4784500e-02 8.9262581e-01]] ============ [0 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0kOhdajtI0",
        "colab_type": "code",
        "outputId": "008573eb-69a7-413c-d41c-2521727f91ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tag_labels = enc.classes_\n",
        "\"\"\"\n",
        "tag_labels >>>>> \n",
        "array(['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental',\n",
        "       'International', 'Pop', 'Rock'], dtype='<U13')\n",
        "\"\"\"\n",
        "predicted_tags = tag_labels[np.argmax(y_pred)]\n",
        "print (predicted_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr0fRgSOkiXF",
        "colab_type": "code",
        "outputId": "082eab51-30bf-4d11-8ae4-9f33c97d41de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "_predicted_tags = tag_labels[np.argmax(y_train[200])]\n",
        "print (_predicted_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw7hya3JsJ2p",
        "colab_type": "text"
      },
      "source": [
        "# EXTRACTION FEATURES FROM AUDIO "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snavfTJ9YWyu",
        "colab_type": "code",
        "outputId": "294fb910-5b00-426c-e079-507ccfe88316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import os\n",
        "import multiprocessing\n",
        "import warnings\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import librosa\n",
        "\n",
        "\n",
        "# the fonction returns the columns how the features.csv is structured \n",
        "def columns():\n",
        "    feature_sizes = dict(chroma_stft=12, chroma_cqt=12, chroma_cens=12,\n",
        "                         tonnetz=6, mfcc=20, rmse=1, zcr=1,\n",
        "                         spectral_centroid=1, spectral_bandwidth=1,\n",
        "                         spectral_contrast=7, spectral_rolloff=1)\n",
        "    moments = ('mean', 'std', 'skew', 'kurtosis', 'median', 'min', 'max')\n",
        "\n",
        "    columns = []\n",
        "    for name, size in feature_sizes.items():\n",
        "        for moment in moments:\n",
        "            it = ((name, moment, '{:02d}'.format(i+1)) for i in range(size))\n",
        "            columns.extend(it)\n",
        "\n",
        "    names = ('feature', 'statistics', 'number')\n",
        "    columns = pd.MultiIndex.from_tuples(columns, names=names)\n",
        "\n",
        "    # More efficient to slice if indexes are sorted.\n",
        "    return columns.sort_values()\n",
        "\n",
        "\n",
        "columns()    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiIndex([('chroma_cens', 'kurtosis', '01'),\n",
              "            ('chroma_cens', 'kurtosis', '02'),\n",
              "            ('chroma_cens', 'kurtosis', '03'),\n",
              "            ('chroma_cens', 'kurtosis', '04'),\n",
              "            ('chroma_cens', 'kurtosis', '05'),\n",
              "            ('chroma_cens', 'kurtosis', '06'),\n",
              "            ('chroma_cens', 'kurtosis', '07'),\n",
              "            ('chroma_cens', 'kurtosis', '08'),\n",
              "            ('chroma_cens', 'kurtosis', '09'),\n",
              "            ('chroma_cens', 'kurtosis', '10'),\n",
              "            ...\n",
              "            (    'tonnetz',      'std', '04'),\n",
              "            (    'tonnetz',      'std', '05'),\n",
              "            (    'tonnetz',      'std', '06'),\n",
              "            (        'zcr', 'kurtosis', '01'),\n",
              "            (        'zcr',      'max', '01'),\n",
              "            (        'zcr',     'mean', '01'),\n",
              "            (        'zcr',   'median', '01'),\n",
              "            (        'zcr',      'min', '01'),\n",
              "            (        'zcr',     'skew', '01'),\n",
              "            (        'zcr',      'std', '01')],\n",
              "           names=['feature', 'statistics', 'number'], length=518)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvpEfJ8fsHmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_features(filepath):\n",
        "\n",
        "    features = pd.Series(index=columns(), dtype=np.float32, name=filepath)\n",
        "\n",
        "    # Catch warnings as exceptions (audioread leaks file descriptors).\n",
        "    warnings.filterwarnings('error', module='librosa')\n",
        "\n",
        "    def feature_stats(name, values):\n",
        "        features[name, 'mean'] = np.mean(values, axis=1)\n",
        "        features[name, 'std'] = np.std(values, axis=1)\n",
        "        features[name, 'skew'] = stats.skew(values, axis=1)\n",
        "        features[name, 'kurtosis'] = stats.kurtosis(values, axis=1)\n",
        "        features[name, 'median'] = np.median(values, axis=1)\n",
        "        features[name, 'min'] = np.min(values, axis=1)\n",
        "        features[name, 'max'] = np.max(values, axis=1)\n",
        "\n",
        "    try:\n",
        "        #filepath = utils.get_audio_path(os.environ.get('AUDIO_DIR'), tid)\n",
        "        x, sr = librosa.load(filepath, sr=None, mono=True)  # kaiser_fast\n",
        "\n",
        "        f = librosa.feature.zero_crossing_rate(x, frame_length=2048, hop_length=512)\n",
        "        feature_stats('zcr', f)\n",
        "\n",
        "        cqt = np.abs(librosa.cqt(x, sr=sr, hop_length=512, bins_per_octave=12,\n",
        "                                 n_bins=7*12, tuning=None))\n",
        "        assert cqt.shape[0] == 7 * 12\n",
        "        assert np.ceil(len(x)/512) <= cqt.shape[1] <= np.ceil(len(x)/512)+1\n",
        "\n",
        "        f = librosa.feature.chroma_cqt(C=cqt, n_chroma=12, n_octaves=7)\n",
        "        feature_stats('chroma_cqt', f)\n",
        "        f = librosa.feature.chroma_cens(C=cqt, n_chroma=12, n_octaves=7)\n",
        "        feature_stats('chroma_cens', f)\n",
        "        f = librosa.feature.tonnetz(chroma=f)\n",
        "        feature_stats('tonnetz', f)\n",
        "\n",
        "        del cqt\n",
        "        stft = np.abs(librosa.stft(x, n_fft=2048, hop_length=512))\n",
        "        assert stft.shape[0] == 1 + 2048 // 2\n",
        "        assert np.ceil(len(x)/512) <= stft.shape[1] <= np.ceil(len(x)/512)+1\n",
        "        del x\n",
        "\n",
        "        f = librosa.feature.chroma_stft(S=stft**2, n_chroma=12)\n",
        "        feature_stats('chroma_stft', f)\n",
        "\n",
        "        f = librosa.feature.rmse(S=stft)\n",
        "        feature_stats('rmse', f)\n",
        "\n",
        "        f = librosa.feature.spectral_centroid(S=stft)\n",
        "        feature_stats('spectral_centroid', f)\n",
        "        f = librosa.feature.spectral_bandwidth(S=stft)\n",
        "        feature_stats('spectral_bandwidth', f)\n",
        "        f = librosa.feature.spectral_contrast(S=stft, n_bands=6)\n",
        "        feature_stats('spectral_contrast', f)\n",
        "        f = librosa.feature.spectral_rolloff(S=stft)\n",
        "        feature_stats('spectral_rolloff', f)\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n",
        "        del stft\n",
        "        f = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n",
        "        feature_stats('mfcc', f)\n",
        "\n",
        "    except Exception as e:\n",
        "        print('{}: {}'.format(filepath, repr(e)))\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfEqKvP2zCs1",
        "colab_type": "code",
        "outputId": "4c575862-31f3-479c-b846-ecacacdf0f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "compute_features(\"/content/drive/My Drive/projet IA/000002.mp3\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature      statistics  number\n",
              "chroma_cens  kurtosis    01        5.619328\n",
              "                         02       -0.464132\n",
              "                         03        1.627383\n",
              "                         04       -0.274711\n",
              "                         05        0.190102\n",
              "                                     ...   \n",
              "zcr          mean        01        0.081016\n",
              "             median      01        0.079102\n",
              "             min         01        0.028320\n",
              "             skew        01        0.908157\n",
              "             std         01        0.020830\n",
              "Name: /content/drive/My Drive/projet IA/000010.mp3, Length: 518, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0VlxF7W0ITZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "array_2 = compute_features(\"/content/drive/My Drive/projet IA/000005.mp3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eZsz0q0xtOP",
        "colab_type": "code",
        "outputId": "970e572b-24f3-45bf-9de5-f6f0de166e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "array_2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature      statistics  number\n",
              "chroma_cens  kurtosis    01       -0.171996\n",
              "                         02        0.226217\n",
              "                         03       -0.099524\n",
              "                         04        0.325517\n",
              "                         05        1.742335\n",
              "                                     ...   \n",
              "zcr          mean        01        0.059330\n",
              "             median      01        0.047852\n",
              "             min         01        0.002930\n",
              "             skew        01        2.091125\n",
              "             std         01        0.047794\n",
              "Name: /content/drive/My Drive/projet IA/000005.mp3, Length: 518, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGjF0Ex819Q0",
        "colab_type": "code",
        "outputId": "e1303a23-2b4e-4a28-ba2c-72abe4f83390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "#transformer la serie array_2 en dataframe\n",
        "df_2 = pd.DataFrame(array_2).transpose()\n",
        "#print(\"transpose\\n\",df_2)\n",
        "\n",
        "# pour enfin appliquer la transformation a df_2\n",
        "\"\"\"\n",
        "_scaler = MinMaxScaler()\n",
        "_features = pd.read_csv(\"/content/drive/My Drive/projet IA/features.csv\", header=[0, 1, 2], index_col=0)\n",
        "_features[_features.columns] = _scaler.fit_transform(_features[_features.columns])\n",
        "\"\"\"\n",
        "loaded_scaler = pickle.load(open('/content/drive/My Drive/projet IA/_saved_scaler','rb'))\n",
        "\n",
        "#obj = _scaler.fit(_features)\n",
        "\n",
        "df_2[df_2.columns] = loaded_scaler.transform(df_2[df_2.columns])\n",
        "df_2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>feature</th>\n",
              "      <th colspan=\"40\" halign=\"left\">chroma_cens</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"33\" halign=\"left\">tonnetz</th>\n",
              "      <th colspan=\"7\" halign=\"left\">zcr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>statistics</th>\n",
              "      <th colspan=\"12\" halign=\"left\">kurtosis</th>\n",
              "      <th colspan=\"12\" halign=\"left\">max</th>\n",
              "      <th colspan=\"12\" halign=\"left\">mean</th>\n",
              "      <th colspan=\"4\" halign=\"left\">median</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"3\" halign=\"left\">max</th>\n",
              "      <th colspan=\"6\" halign=\"left\">mean</th>\n",
              "      <th colspan=\"6\" halign=\"left\">median</th>\n",
              "      <th colspan=\"6\" halign=\"left\">min</th>\n",
              "      <th colspan=\"6\" halign=\"left\">skew</th>\n",
              "      <th colspan=\"6\" halign=\"left\">std</th>\n",
              "      <th>kurtosis</th>\n",
              "      <th>max</th>\n",
              "      <th>mean</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>skew</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>07</th>\n",
              "      <th>08</th>\n",
              "      <th>09</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>...</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>02</th>\n",
              "      <th>03</th>\n",
              "      <th>04</th>\n",
              "      <th>05</th>\n",
              "      <th>06</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "      <th>01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>/content/drive/My Drive/projet IA/000005.mp3</th>\n",
              "      <td>0.036528</td>\n",
              "      <td>0.025035</td>\n",
              "      <td>0.003326</td>\n",
              "      <td>0.074296</td>\n",
              "      <td>0.077977</td>\n",
              "      <td>0.043213</td>\n",
              "      <td>0.007897</td>\n",
              "      <td>0.003915</td>\n",
              "      <td>0.016511</td>\n",
              "      <td>0.037991</td>\n",
              "      <td>0.003543</td>\n",
              "      <td>0.02163</td>\n",
              "      <td>0.339923</td>\n",
              "      <td>0.671373</td>\n",
              "      <td>0.484825</td>\n",
              "      <td>0.426509</td>\n",
              "      <td>0.493014</td>\n",
              "      <td>0.425242</td>\n",
              "      <td>0.629209</td>\n",
              "      <td>0.707196</td>\n",
              "      <td>0.786801</td>\n",
              "      <td>0.725206</td>\n",
              "      <td>0.549584</td>\n",
              "      <td>0.409815</td>\n",
              "      <td>0.356635</td>\n",
              "      <td>0.456826</td>\n",
              "      <td>0.371178</td>\n",
              "      <td>0.343225</td>\n",
              "      <td>0.384425</td>\n",
              "      <td>0.38441</td>\n",
              "      <td>0.543157</td>\n",
              "      <td>0.517335</td>\n",
              "      <td>0.588718</td>\n",
              "      <td>0.511547</td>\n",
              "      <td>0.422879</td>\n",
              "      <td>0.344003</td>\n",
              "      <td>0.349763</td>\n",
              "      <td>0.446009</td>\n",
              "      <td>0.371425</td>\n",
              "      <td>0.361412</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197405</td>\n",
              "      <td>0.086982</td>\n",
              "      <td>0.075099</td>\n",
              "      <td>0.495989</td>\n",
              "      <td>0.249435</td>\n",
              "      <td>0.500777</td>\n",
              "      <td>0.500805</td>\n",
              "      <td>0.464786</td>\n",
              "      <td>0.411575</td>\n",
              "      <td>0.511783</td>\n",
              "      <td>0.257602</td>\n",
              "      <td>0.542187</td>\n",
              "      <td>0.48634</td>\n",
              "      <td>0.501166</td>\n",
              "      <td>0.456919</td>\n",
              "      <td>0.851952</td>\n",
              "      <td>0.836562</td>\n",
              "      <td>0.610535</td>\n",
              "      <td>0.691623</td>\n",
              "      <td>0.936226</td>\n",
              "      <td>0.832608</td>\n",
              "      <td>0.559563</td>\n",
              "      <td>0.470434</td>\n",
              "      <td>0.434492</td>\n",
              "      <td>0.584266</td>\n",
              "      <td>0.553307</td>\n",
              "      <td>0.575621</td>\n",
              "      <td>0.162334</td>\n",
              "      <td>0.072098</td>\n",
              "      <td>0.205574</td>\n",
              "      <td>0.078128</td>\n",
              "      <td>0.102836</td>\n",
              "      <td>0.096453</td>\n",
              "      <td>0.005037</td>\n",
              "      <td>0.29798</td>\n",
              "      <td>0.089176</td>\n",
              "      <td>0.063844</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.133734</td>\n",
              "      <td>0.143396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 518 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "feature                                      chroma_cens  ...       zcr\n",
              "statistics                                      kurtosis  ...       std\n",
              "number                                                01  ...        01\n",
              "/content/drive/My Drive/projet IA/000005.mp3    0.036528  ...  0.143396\n",
              "\n",
              "[1 rows x 518 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVt4ieXR3b1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "_pca = PCA(n_components=0.95)\n",
        "new_X = _pca.fit_transform(_features)\n",
        "# Apply dimensionality reduction to X.\n",
        "# X is projected on the first principal components previously extracted from a training set.\n",
        "df_2_pca = _pca.transform(df_2)\n",
        "\"\"\"\n",
        "_pca = pickle.load(open('/content/drive/My Drive/projet IA/_saved_pca','rb'))\n",
        "df_2_pca = _pca.transform(df_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7RGeVhS4jJp",
        "colab_type": "code",
        "outputId": "8bef01a3-5abc-423e-c074-5e47345087b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#pd.DataFrame(array_2).transpose()\n",
        "df_2_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 148)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XLdx-CY1XZC",
        "colab_type": "code",
        "outputId": "f8b36cf3-66a4-44ce-bca9-07e00a10ac24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_pred = model_load.predict(df_2_pca)\n",
        "\n",
        "loaded_enc = pickle.load(open('/content/drive/My Drive/projet IA/_saved_enc','rb'))\n",
        "#df_2_pca = _pca.transform(df_2)\n",
        "tag_labels = loaded_enc.classes_\n",
        "\"\"\"\n",
        "tag_labels >>>>> return \n",
        "array(['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental',\n",
        "       'International', 'Pop', 'Rock'], dtype='<U13')\n",
        "\"\"\"\n",
        "\n",
        "# np.argmax(_list) Returns the indices of the maximum values along an axis.\n",
        "predicted_tags = tag_labels[np.argmax(y_pred)]\n",
        "print (predicted_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hip-Hop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CJXSaeK9kkv",
        "colab_type": "code",
        "outputId": "7625258c-32ce-4c52-9723-e385190e7acf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "tag_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental',\n",
              "       'International', 'Pop', 'Rock'], dtype='<U13')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIRgCg1Q99Gx",
        "colab_type": "code",
        "outputId": "94d4633b-b72e-4d3e-a0d8-6b69afe7aa81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genres</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>track_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>Hip-Hop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>21</td>\n",
              "      <td>Hip-Hop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>17</td>\n",
              "      <td>Folk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>17</td>\n",
              "      <td>Folk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154308</th>\n",
              "      <td>21</td>\n",
              "      <td>Hip-Hop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154309</th>\n",
              "      <td>21</td>\n",
              "      <td>Hip-Hop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154413</th>\n",
              "      <td>10</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154414</th>\n",
              "      <td>10</td>\n",
              "      <td>Pop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155066</th>\n",
              "      <td>21</td>\n",
              "      <td>Hip-Hop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          genres    title\n",
              "track_id                 \n",
              "2             21  Hip-Hop\n",
              "5             21  Hip-Hop\n",
              "10            10      Pop\n",
              "140           17     Folk\n",
              "141           17     Folk\n",
              "...          ...      ...\n",
              "154308        21  Hip-Hop\n",
              "154309        21  Hip-Hop\n",
              "154413        10      Pop\n",
              "154414        10      Pop\n",
              "155066        21  Hip-Hop\n",
              "\n",
              "[8000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    }
  ]
}